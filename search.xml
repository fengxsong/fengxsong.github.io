<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>如何基于CRD开发一个Controller</title>
      <link href="/2018/04/27/%E5%A6%82%E4%BD%95%E5%9F%BA%E4%BA%8ECRD%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AAController/"/>
      <url>/2018/04/27/%E5%A6%82%E4%BD%95%E5%9F%BA%E4%BA%8ECRD%E5%BC%80%E5%8F%91%E4%B8%80%E4%B8%AAController/</url>
      <content type="html"><![CDATA[<h1 id="Let’s-get-started"><a href="#Let’s-get-started" class="headerlink" title="Let’s get started"></a>Let’s get started</h1><blockquote><p>以下例子基于<a href="github.com/bitnami-labs/helm-crd">helm-crd</a>仅做了部分修改。</p></blockquote><h2 id="创建CRD"><a href="#创建CRD" class="headerlink" title="创建CRD"></a>创建CRD</h2><p>先定义<code>types.go</code>，路径为<code>pkg/apis/helm.bitnami.com/v1/</code>，注意不要少了注解 (:</p><figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="comment">// +genclient</span></span><br><span class="line"><span class="comment">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> HelmRelease <span class="keyword">struct</span> &#123;</span><br><span class="line">    metav1.TypeMeta   <span class="string">`json:",inline"`</span></span><br><span class="line">    metav1.ObjectMeta <span class="string">`json:"metadata,omitempty"`</span></span><br><span class="line"></span><br><span class="line">    Spec   HelmReleaseSpec   <span class="string">`json:"spec"`</span></span><br><span class="line">    Status HelmReleaseStatus <span class="string">`json:"status"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> HelmReleaseSpec <span class="keyword">struct</span> &#123;</span><br><span class="line">    RepoURL     <span class="keyword">string</span> <span class="string">`json:"repoUrl,omitempty"`</span></span><br><span class="line">    ChartName   <span class="keyword">string</span> <span class="string">`json:"chartName,omitempty"`</span></span><br><span class="line">    Version     <span class="keyword">string</span> <span class="string">`json:"version,omitempty"`</span></span><br><span class="line">    Values      <span class="keyword">string</span> <span class="string">`json:"values,omitempty"`</span></span><br><span class="line">    Force       <span class="keyword">bool</span>   <span class="string">`json:"force,omitempty"`</span></span><br><span class="line">    Recreate    <span class="keyword">bool</span>   <span class="string">`json:"recreate,omitempty"`</span></span><br><span class="line">    Paused      <span class="keyword">bool</span>   <span class="string">`json:"paused,omitempty"`</span></span><br><span class="line">    Description <span class="keyword">string</span> <span class="string">`json:"description,omitempty"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> HelmRealeasePhase <span class="keyword">string</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">    HelmRealeasePhaseUnknown HelmRealeasePhase = <span class="string">""</span></span><br><span class="line">    HelmRealeasePhaseNew     HelmRealeasePhase = <span class="string">"New"</span></span><br><span class="line">    HelmRealeasePhaseReady   HelmRealeasePhase = <span class="string">"Ready"</span></span><br><span class="line">    HelmRealeasePhaseFailed  HelmRealeasePhase = <span class="string">"Failed"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> HelmReleaseStatus <span class="keyword">struct</span> &#123;</span><br><span class="line">    Config   <span class="keyword">string</span>            <span class="string">`json:"config,omitempty"`</span></span><br><span class="line">    Phase    HelmRealeasePhase <span class="string">`json:"phase"`</span></span><br><span class="line">    Revision <span class="keyword">int32</span>             <span class="string">`json:"revision,omitempty"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> HelmReleaseList <span class="keyword">struct</span> &#123;</span><br><span class="line">    metav1.TypeMeta <span class="string">`json:",inline"`</span></span><br><span class="line">    metav1.ListMeta <span class="string">`json:"metadata"`</span></span><br><span class="line">    Items           []HelmRelease <span class="string">`json:"items"`</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>之后使用<code>code-generator</code>生成<code>client</code>/<code>apis</code>的代码</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vendor/k8s.io/code-generator/generate-groups.sh \</span><br><span class="line">    all \</span><br><span class="line">    github.com/fengxsong/helm-crd/pkg/client \</span><br><span class="line">    github.com/fengxsong/helm-crd/pkg/apis \</span><br><span class="line">    helm.bitnami.com:v1</span><br></pre></td></tr></table></figure><p>那么接下来我们只需要专注于controller的实现即可。</p><h2 id="Controller实现"><a href="#Controller实现" class="headerlink" title="Controller实现"></a>Controller实现</h2><h3 id="创建controller"><a href="#创建controller" class="headerlink" title="创建controller"></a>创建controller</h3><p>controller的定义</p><figure class="highlight golang"><table><tr><td class="code"><pre><span class="line">    <span class="keyword">type</span> Controller <span class="keyword">struct</span> &#123;</span><br><span class="line">        <span class="comment">// "k8s.io/client-go/kubernetes" 连接apiserver操作k8s内的objects</span></span><br><span class="line">        kubeClientset kubernetes.Interface</span><br><span class="line">        <span class="comment">// "github.com/fengxsong/helm-crd/pkg/client/clientset/versioned" code-generator生成，主要调用HelmReleaseInterface</span></span><br><span class="line">        clientset     helmClientset.Interface</span><br><span class="line">        <span class="comment">// "k8s.io/helm/pkg/helm" 调用集群内的helm tiller server</span></span><br><span class="line">        helmClient    *helm.Client</span><br><span class="line">        <span class="comment">// "k8s.io/client-go/tools/cache" informer去ListWatch helmreleases的 add/update/delete 事件</span></span><br><span class="line">        informer      cache.SharedIndexInformer</span><br><span class="line">        <span class="comment">// "k8s.io/client-go/util/workqueue" 限流</span></span><br><span class="line">        queue         workqueue.RateLimitingInterface</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="NewController函数"><a href="#NewController函数" class="headerlink" title="NewController函数"></a><code>NewController</code>函数</h4><figure class="highlight golang"><table><tr><td class="code"><pre><span class="line">c := &amp;Controller&#123;</span><br><span class="line">    kubeClientset: kubeClientset,</span><br><span class="line">    clientset:     clientset,</span><br><span class="line">    helmClient:    helm.NewClient(helm.Host(settings.TillerHost), helm.ConnectTimeout(<span class="number">5</span>)),</span><br><span class="line">    informer:      crdInformersFactory.Helm().V1().HelmReleases().Informer(),</span><br><span class="line">    queue:         workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), <span class="string">""</span>),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>informer注册事件回调</p><figure class="highlight golang"><table><tr><td class="code"><pre><span class="line">   <span class="comment">// 回调函数都将key添加到c.queue中</span></span><br><span class="line">c.informer.AddEventHandler(cache.ResourceEventHandlerFuncs&#123;</span><br><span class="line">       <span class="comment">// onAddFunc检查当.status.phase为Ready时直接return</span></span><br><span class="line">       AddFunc:    c.onAddFunc,</span><br><span class="line">       <span class="comment">// onUpdateFunc检查新旧objects的resourceVersion是否相等，当相等时直接return</span></span><br><span class="line">       UpdateFunc: c.onUpdateFunc,</span><br><span class="line">       <span class="comment">// </span></span><br><span class="line">DeleteFunc: c.onDeleteFunc,</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h4 id="Run"><a href="#Run" class="headerlink" title="Run"></a>Run</h4><p>即实际运行controller的函数</p><ul><li>goroutine处理crash</li><li>goroutine处理sigterm事件</li><li>创建helm客户端所需要初始的配置目录</li><li>等待更新informer本地的indexer缓存，之后将事件distribute给所有的listerner</li><li><code>Until loops</code>运行<code>runWorker</code>函数，当<code>stop channel</code>关闭时退出</li><li><code>runWorker</code> for look 调用<code>processNextItem</code></li><li><code>processNextItem</code>从c.queue中Pop消费事件，交由updateRelease处理或失败后重试或忽略</li></ul><h4 id="updateRelease"><a href="#updateRelease" class="headerlink" title="updateRelease"></a>updateRelease</h4><ul><li>从informer indexer里取出key，假定它的状态为不存在，那这个理解为是Delete事件即helmrelease被删除了，调用helmClient.DeleteRelease</li><li>添加Paused，可以是操作helmrelease需要审核的情况，当为true时则暂时不采取操作</li><li>从.spec.RepoUrl + .spec.Chart + .spec.Version下载chart压缩包到cache文件夹内</li><li>helmClient检查以当前helmrelease.Name的名字的Release，当不存在则调用 helmClient.InstallReleaseFromChart，当存在则调用 helmClient.UpdateReleaseFromChart</li><li>install/update完成后查询对应的Release的信息，更新.status的值，之后又触发Update事件，所以我们需要在onUpdateFunc函数里判断防止重复更新<br>据说1.10的subResources添加了status，但是貌似测试无效即仍会出发Update，还是说姿势有问题？</li></ul><p>以上的操作实际上是类似于手动执行helm cli，所有object的操作信息会在tiller服务端被处理，编排操作交给tiller服务端。</p>]]></content>
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> crd </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>持续更新笔记</title>
      <link href="/2018/04/26/%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/04/26/%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E7%AC%94%E8%AE%B0/</url>
      <content type="html"><![CDATA[<ul><li><p>k8s限制带宽 <code>kubernetes.io/ingress-bandwidth</code></p></li><li><p>prometheus query查询出来的结果假如是null的话，grafana在图表显示会是no point，我们需要把null的结果默认显示为0，treat null as zero似乎不管用</p></li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">QUERY **OR vector(0)**</span><br></pre></td></tr></table></figure><ul><li>ETCD查询所有key值</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ETCDCTL_API=3 etcdctl get <span class="string">""</span> --prefix=<span class="literal">true</span></span><br><span class="line">ETCDCTL_API=3 etcdctl get <span class="string">""</span> --from-key</span><br></pre></td></tr></table></figure><ul><li>k8s的pod设置<code>hostNetwork: true</code>的时候默认<code>dnsPolicy</code>还是<code>ClusterFirst</code> ，导致这个pod不能解析内部的service，那么需要把dnsPolicy设置为<code>ClusterFirstWithHostNet</code></li><li>k8s使用kubeadm起来的时候集群默认是不会schedule pods去master节点的，假如我们需要schedule pods on the master</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl taint nodes --all node-role.kubernetes.io/master-</span><br><span class="line"><span class="comment">#OR</span></span><br><span class="line">kubectl taint nodes k8s-master02 node-role.kubernetes.io/master-</span><br></pre></td></tr></table></figure><p>假设需要再次设置不允许pod运行在master上</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl taint nodes k8s-master02 key=node-role.kubernetes.io/master:NoSchedule</span><br></pre></td></tr></table></figure><p><a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/" target="_blank" rel="noopener">taint-and-toleration</a></p>]]></content>
      
      
    </entry>
    
    <entry>
      <title>CephRBD是如何存储数据的</title>
      <link href="/2018/04/23/CephRBD%E6%98%AF%E5%A6%82%E4%BD%95%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%E7%9A%84/"/>
      <url>/2018/04/23/CephRBD%E6%98%AF%E5%A6%82%E4%BD%95%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%E7%9A%84/</url>
      <content type="html"><![CDATA[<h2 id="列出pools"><a href="#列出pools" class="headerlink" title="列出pools"></a>列出pools</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">~$: sudo ceph osd lspools</span><br><span class="line">s0 rbd,1 kube</span><br></pre></td></tr></table></figure><h2 id="查找池使用的replication-level"><a href="#查找池使用的replication-level" class="headerlink" title="查找池使用的replication level"></a>查找池使用的<code>replication level</code></h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">~$: sudo ceph osd dump | grep -i kube</span><br><span class="line">pool 1 <span class="string">'kube'</span> replicated size 2 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 384 pgp_num 384 last_change 38 flags hashpspool stripe_width 0</span><br></pre></td></tr></table></figure><p>可以看到当前replicas的size大小为2，当然我们可以重新配置这个值</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">~$: ceph osd pool <span class="built_in">set</span> kube size 3</span><br></pre></td></tr></table></figure><h2 id="查看kube-pool包含的objects"><a href="#查看kube-pool包含的objects" class="headerlink" title="查看kube pool包含的objects"></a>查看<code>kube pool</code>包含的objects</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">~$: sudo rados -p kube ls</span><br></pre></td></tr></table></figure><h2 id="PG-Placement-Group"><a href="#PG-Placement-Group" class="headerlink" title="PG (Placement Group)"></a>PG (Placement Group)</h2><p>Ceph集群将object对象与PG映射，PG包含了分布在不同osd节点的objects，来提高可用性。</p><h2 id="Object"><a href="#Object" class="headerlink" title="Object"></a>Object</h2><p>在ceph集群里数据存储的最小单位，任意一个东西都是以object的形式保存。</p><h2 id="查看object是关联到哪个PG以及是存储在哪里"><a href="#查看object是关联到哪个PG以及是存储在哪里" class="headerlink" title="查看object是关联到哪个PG以及是存储在哪里"></a>查看<code>object</code>是关联到哪个<code>PG</code>以及是存储在哪里</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">~$: sudo ceph osd map kube rbd_id.kubernetes-dynamic-pvc-3e7d3f6b-d0cd-11e7-ba9d-0a580af40319</span><br><span class="line">osdmap e12056 pool <span class="string">'kube'</span> (1) object <span class="string">'rbd_id.kubernetes-dynamic-pvc-3e7d3f6b-d0cd-11e7-ba9d-0a580af40319'</span> -&gt; pg 1.493b5d72 (1.172) -&gt; up ([3,6], p3) acting ([3,6], p3)</span><br></pre></td></tr></table></figure><p>以上提供的信息有<br><code>object</code>的<code>version id</code>为<code>e12056</code>，属于<code>kube</code>pool，<code>kube pool</code>的id为<code>1</code>，<code>object</code>的<code>id</code>为<code>rbd_id.kubernetes-dynamic-pvc-3e7d3f6b-d0cd-11e7-ba9d-0a580af40319</code>，pg属于<code>1.172</code>，在<code>osd.3</code>以及<code>osd.6</code>上。</p><p>接下来可以去具体的osd节点上查看，可以先从<code>sudo ceph osd tree</code>查看osd节点对应的机器名。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">~$: sudo du -sh /u01/current/1.172_head/</span><br><span class="line">163M /u01/current/1.172_head/</span><br></pre></td></tr></table></figure><p>综上我们可以总结出：</p><ul><li>ceph存储集群可以有不止一个池</li><li>每个池应该有多个pg</li><li>一个pg包含多个object</li><li>一个pg会分布到不同的osd节点，且object也是分布到不同的osd节点，映射到pg的第一个osd节点将是主osd，其他为备节点</li><li>一个object可以被映射到一个pg</li></ul><p>一个池需要多少个pg由公式计算出</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">           (OSDs * 100)</span><br><span class="line">Total PGs = ------------</span><br><span class="line">              Replicas</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">~$: sudo ceph osd <span class="built_in">stat</span></span><br><span class="line">osdmap e12056: 5 osds: 5 up, 5 <span class="keyword">in</span></span><br><span class="line">flags sortbitwise,require_jewel_osds</span><br></pre></td></tr></table></figure><p>公式计算 5*100/2 = 250，再以2的幂等数对比，当replicas size=2，osd size=5的时候，整个集群中共需要有 256 个pg</p><p><a href="https://ceph.com/geen-categorie/how-data-is-stored-in-ceph-cluster/" target="_blank" rel="noopener">how-data-is-stored-in-ceph-cluster</a></p>]]></content>
      
      <categories>
          
          <category> ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ceph </tag>
            
            <tag> rbd </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>K8s集群升级</title>
      <link href="/2018/04/10/K8s%E9%9B%86%E7%BE%A4%E5%8D%87%E7%BA%A7/"/>
      <url>/2018/04/10/K8s%E9%9B%86%E7%BE%A4%E5%8D%87%E7%BA%A7/</url>
      <content type="html"><![CDATA[<h1 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1><h2 id="kubeadm"><a href="#kubeadm" class="headerlink" title="kubeadm"></a>kubeadm</h2><h3 id="手动安装kubeadm新版本"><a href="#手动安装kubeadm新版本" class="headerlink" title="手动安装kubeadm新版本"></a>手动安装<code>kubeadm</code>新版本</h3><p>这里是安装1.10.x以上版本，下面实际上升级的是1.9.7版本</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> VERSION=$(curl -sSL https://dl.k8s.io/release/stable.txt) </span><br><span class="line"><span class="built_in">export</span> ARCH=amd64 <span class="comment"># or: arm, arm64, ppc64le, s390x</span></span><br><span class="line">curl -sSL https://dl.k8s.io/release/<span class="variable">$&#123;VERSION&#125;</span>/bin/linux/<span class="variable">$&#123;ARCH&#125;</span>/kubeadm &gt; /tmp/kubeadm</span><br><span class="line">chmod a+rx /tmp/kubeadm</span><br></pre></td></tr></table></figure><h3 id="执行检查升级版本计划"><a href="#执行检查升级版本计划" class="headerlink" title="执行检查升级版本计划"></a>执行检查升级版本计划</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo /tmp/kubeadm upgrade plan</span><br></pre></td></tr></table></figure><p>preflight check不通过，报错如下</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[upgrade/config] FATAL: could not decode configuration: unable to decode config from bytes: v1alpha1.MasterConfiguration: KubeProxy: v1alpha1.KubeProxy: Config: v1alpha1.KubeProxyConfiguration: FeatureGates: ReadMapCB: expect &#123; or n, but found <span class="string">", error found in #10 byte of ...|reGates"</span>:<span class="string">""</span>,<span class="string">"healthz|..., bigger context ...|24h0m0s"</span>&#125;,<span class="string">"enableProfiling"</span>:<span class="literal">false</span>,<span class="string">"featureGates"</span>:<span class="string">""</span>,<span class="string">"healthzBindAddress"</span>:<span class="string">"0.0.0.0:10256"</span>,<span class="string">"hostnameOv|...</span></span><br></pre></td></tr></table></figure><p><code>KubeProxyConfiguration的featureGates</code>由string类型更换成了map类型，那么通过手动修改当前的<code>MasterConfiguration.kubeProxy.config.featureGates</code>为{}即可。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl -n kube-system edit cm kubeadm-config</span><br></pre></td></tr></table></figure><p>当前使用<a href="https://github.com/fengxsong/kubeadm-ha" target="_blank" rel="noopener">kubeadm-ha</a>安装的k8s集群使用了外部的etcd，但是在<code>cmd/kubeadm/app/cmd/upgrade/plan.go</code>的仍然是指定了内部的etcd集群</p><figure class="highlight golang"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">RunPlan</span><span class="params">(parentFlags *cmdUpgradeFlags)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"><span class="comment">// Start with the basics, verify that the cluster is healthy, build a client and a versionGetter. Never dry-run when planning.</span></span><br><span class="line">upgradeVars, err := enforceRequirements(parentFlags, <span class="literal">false</span>, <span class="string">""</span>)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Define Local Etcd cluster to be able to retrieve information</span></span><br><span class="line">etcdCluster := kubeadmutil.LocalEtcdCluster&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Compute which upgrade possibilities there are</span></span><br><span class="line">availUpgrades, err := upgrade.GetAvailableUpgrades(upgradeVars.versionGetter, parentFlags.allowExperimentalUpgrades, parentFlags.allowRCUpgrades, etcdCluster, upgradeVars.cfg.FeatureGates)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> fmt.Errorf(<span class="string">"[upgrade/versions] FATAL: %v"</span>, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Tell the user which upgrades are available</span></span><br><span class="line">printAvailableUpgrades(availUpgrades, os.Stdout, upgradeVars.cfg.FeatureGates)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最新版本在upgrade时未能手动指定或是读取masterConfiguration的etcd的配置来连接外部的etcd集群，<a href="https://github.com/kubernetes/kubeadm/issues/727" target="_blank" rel="noopener">待解决</a>。</p><h2 id="软件包管理（可选）"><a href="#软件包管理（可选）" class="headerlink" title="软件包管理（可选）"></a>软件包管理（可选）</h2><p>切换使用Aliyun镜像仓库，首先先把之前的引用官方的去掉。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rm -f /etc/apt/sources.list.d/&#123;docker.list,kubernetes.list&#125;</span><br><span class="line"></span><br><span class="line">sudo apt-get update &amp;&amp; sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common</span><br><span class="line">curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -</span><br><span class="line">sudo add-apt-repository <span class="string">"deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu <span class="variable">$(lsb_release -cs)</span> stable"</span></span><br><span class="line">curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add - </span><br><span class="line">sudo add-apt-repository <span class="string">"deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main"</span></span><br><span class="line"><span class="comment"># 更新并安装指定版本</span></span><br><span class="line">sudo apt-get -y update</span><br><span class="line">sudo apt-get -y install docker-ce=17.03.2~ce-0~ubuntu-xenial</span><br><span class="line">sudo apt-get -y install -y kubelet kubeadm kubectl</span><br></pre></td></tr></table></figure><h2 id="实际升级步骤"><a href="#实际升级步骤" class="headerlink" title="实际升级步骤"></a>实际升级步骤</h2><h3 id="推送镜像到内部仓库"><a href="#推送镜像到内部仓库" class="headerlink" title="推送镜像到内部仓库"></a>推送镜像到内部仓库</h3><blockquote><p>1.9.7版本的kubeadm指定了1.14.7的kube-dns组件,3.1.11版本的etcd，对应版本的信息可以在cmd/kubeadm/app/constants/constants.go找到</p></blockquote><p>假定我们的内部仓库地址为<code>hub.example.io</code></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> image <span class="keyword">in</span> k8s-dns-kube-dns-amd64 k8s-dns-dnsmasq-nanny-amd64 k8s-dns-sidecar-amd64;<span class="keyword">do</span> docker pull gcr.io/google_containers/<span class="variable">$image</span>:1.14.7 &amp;&amp; docker tag gcr.io/google_containers/<span class="variable">$image</span>:1.14.7 hub.example.io/google_containers/<span class="variable">$image</span>:1.14.7 &amp;&amp; docker push hub.example.io/google_containers/<span class="variable">$image</span>:1.14.7;<span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> image <span class="keyword">in</span> kube-apiserver-amd64 kube-controller-manager-amd64 kube-scheduler-amd64 kube-proxy-amd64;<span class="keyword">do</span> docker pull gcr.io/google_containers/<span class="variable">$image</span>:v1.9.7 &amp;&amp; docker tag gcr.io/google_containers/<span class="variable">$image</span>:v1.9.7 hub.example.io/google_containers/<span class="variable">$image</span>:v1.9.7 &amp;&amp;  docker push hub.example.io/google_containers/<span class="variable">$image</span>:v1.9.7;<span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">docker pull gcr.io/google_containers/etcd-amd64:3.1.11 &amp;&amp; docker tag gcr.io/google_containers/etcd-amd64:3.1.11 hub.example.io/google_containers/etcd-amd64:3.1.11 &amp;&amp; docker push hub.example.io/google_containers/etcd-amd64:3.1.11</span><br></pre></td></tr></table></figure><h3 id="添加kubeadm-config-yaml指定镜像使用内部仓库"><a href="#添加kubeadm-config-yaml指定镜像使用内部仓库" class="headerlink" title="添加kubeadm-config.yaml指定镜像使用内部仓库"></a>添加kubeadm-config.yaml指定镜像使用内部仓库</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">imageRepository:</span> <span class="string">hub.example.io/google_containers</span></span><br><span class="line"><span class="attr">kubernetesVersion:</span> <span class="string">v1.9.7</span></span><br><span class="line"><span class="attr">networking:</span></span><br><span class="line"><span class="attr">  podSubnet:</span> <span class="number">10.244</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br></pre></td></tr></table></figure><h3 id="kubeadm升级指定版本"><a href="#kubeadm升级指定版本" class="headerlink" title="kubeadm升级指定版本"></a>kubeadm升级指定版本</h3><p>先手动下载kubeadm二进制包</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> VERSION=v1.9.7</span><br><span class="line"><span class="built_in">export</span> ARCH=amd64</span><br><span class="line">curl -sSL https://dl.k8s.io/release/<span class="variable">$&#123;VERSION&#125;</span>/bin/linux/<span class="variable">$&#123;ARCH&#125;</span>/kubeadm &gt; /usr/<span class="built_in">local</span>/bin/kubeadm</span><br></pre></td></tr></table></figure><p>执行升级集群</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">/usr/<span class="built_in">local</span>/bin/kubeadm upgrade apply v1.9.7 --config kubeadm-config.yaml --dry-run</span><br></pre></td></tr></table></figure><p>先<code>--dry-run</code>查看新版本的manifests是否正确，确认无误后命令去掉<code>--dry-run</code>参数</p><p>k8s组件升级完之后需手动升级kubelet，首先需要驱逐node节点上的pod，在master节点上执行</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl drain node-id-xxx --delete-local-data --ignore-daemonsets</span><br></pre></td></tr></table></figure><p>待完成后才使用包管理工具升级kubelet以及kubeadm</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install kubelet=1.9.7-00 kubeadm=1.9.7-00</span><br></pre></td></tr></table></figure><p>因为docker-ce版本默认的<code>Cgroup Driver</code>为<code>cgroupfs</code>，与<code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code>文件内默认指定的driver不同，删除掉即可。</p><p>最后重启kubelet，node节点重新设置schedule</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl uncordon node-ix-xxx</span><br></pre></td></tr></table></figure><p>一个比较简单的ansible-pb例子<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- hosts: minions</span><br><span class="line">  become: true</span><br><span class="line">  gather_facts: true</span><br><span class="line">  serial: 10%</span><br><span class="line">  tasks:</span><br><span class="line">  - name: Remove node from cluster</span><br><span class="line">    local_action: command &quot;kubectl drain &#123;&#123; inventory_hostname &#125;&#125; --delete-local-data --ignore-daemonsets&quot;</span><br><span class="line"></span><br><span class="line">  - name: Wait for pods is evited</span><br><span class="line">    wait_for: timeout=60</span><br><span class="line">    delegate_to: localhost</span><br><span class="line"></span><br><span class="line">  - name: Install a newer version of kubeadm/kubelet</span><br><span class="line">    apt: name=&quot;&#123;&#123;item&#125;&#125;&quot; state=present</span><br><span class="line">    with_items:</span><br><span class="line">    - kubeadm=1.9.7-00</span><br><span class="line">    - kubelet=1.9.7-00</span><br><span class="line"></span><br><span class="line">  - name: Replace runtime configuration of kubelet</span><br><span class="line">    file: src=10-kubeadm.conf dest=/etc/systemd/system/kubelet.service.d/10-kubeadm.conf mode=0640</span><br><span class="line"></span><br><span class="line">  - name: Restart kubelet</span><br><span class="line">    systemd: name=kubelet state=restarted daemon_reload=yes</span><br><span class="line"></span><br><span class="line">  - name: Rejoin the node into cluster</span><br><span class="line">    local_action: command &quot;kubectl uncordon &#123;&#123; inventory_hostname &#125;&#125;&quot;</span><br></pre></td></tr></table></figure></p>]]></content>
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubeadm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>K8s日志收集</title>
      <link href="/2018/04/01/K8s%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86/"/>
      <url>/2018/04/01/K8s%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86/</url>
      <content type="html"><![CDATA[<p><a href="https://www.elastic.co/guide/en/beats/filebeat/master/running-on-kubernetes.html" target="_blank" rel="noopener">官方文档</a></p><blockquote><p>因为业务出现exception的话会打印多行错误日志，虽然说我们可以通过multiline的方式，但需要日志是符合指定的格式的。线上各种业务的日志格式不一，所以还是将业务日志写在文件里，以hostPath挂载的方式来处理。</p></blockquote><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">filebeat-config</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> <span class="string">filebeat</span></span><br><span class="line">    <span class="string">kubernetes.io/cluster-service:</span> <span class="string">"true"</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="string">filebeat.yml:</span> <span class="string">|-</span></span><br><span class="line">    <span class="string">filebeat.config:</span></span><br><span class="line"><span class="attr">      prospectors:</span></span><br><span class="line">        <span class="comment"># Mounted `filebeat-prospectors` configmap:</span></span><br><span class="line"><span class="attr">        path:</span> <span class="string">$&#123;path.config&#125;/prospectors.d/*.yml</span></span><br><span class="line">        <span class="comment"># Reload prospectors configs as they change:</span></span><br><span class="line">        <span class="string">reload.enabled:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">      modules:</span></span><br><span class="line"><span class="attr">        path:</span> <span class="string">$&#123;path.config&#125;/modules.d/*.yml</span></span><br><span class="line">        <span class="comment"># Reload module configs as they change:</span></span><br><span class="line">        <span class="string">reload.enabled:</span> <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="attr">    processors:</span></span><br><span class="line"><span class="attr">      - add_cloud_metadata:</span></span><br><span class="line"></span><br><span class="line">    <span class="string">output.redis:</span></span><br><span class="line"><span class="attr">      hosts:</span> <span class="string">'$&#123;FILEBEAT_OUTPUT&#125;'</span></span><br><span class="line"><span class="attr">      key:</span> <span class="string">'<span class="template-variable">%&#123;[fields.log_topic]&#125;</span>'</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">filebeat-prospectors</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> <span class="string">filebeat</span></span><br><span class="line">    <span class="string">kubernetes.io/cluster-service:</span> <span class="string">"true"</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="string">kubernetes.yml:</span> <span class="string">|-</span></span><br><span class="line"><span class="attr">    - type:</span> <span class="string">docker</span></span><br><span class="line">      <span class="string">containers.ids:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">"*"</span></span><br><span class="line"><span class="attr">      fields:</span></span><br><span class="line"><span class="attr">        log_topic:</span> <span class="string">filebeat-docker</span></span><br><span class="line"><span class="attr">      processors:</span></span><br><span class="line"><span class="attr">        - add_kubernetes_metadata:</span></span><br><span class="line"><span class="attr">            in_cluster:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    - type:</span> <span class="string">log</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="bullet">      -</span> <span class="string">/logs/*/*.log</span></span><br><span class="line"><span class="attr">      fields:</span></span><br><span class="line"><span class="attr">        log_topic:</span> <span class="string">filebeat-log</span></span><br><span class="line">      <span class="string">multiline.pattern:</span> <span class="string">'^\[[0-9]&#123;4&#125;-[0-9]&#123;2&#125;-[0-9]&#123;2&#125;'</span></span><br><span class="line">      <span class="string">multiline.negate:</span> <span class="literal">true</span></span><br><span class="line">      <span class="string">multiline.match:</span> <span class="string">after</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">filebeat</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> <span class="string">filebeat</span></span><br><span class="line">    <span class="string">kubernetes.io/cluster-service:</span> <span class="string">"true"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        k8s-app:</span> <span class="string">filebeat</span></span><br><span class="line">        <span class="string">kubernetes.io/cluster-service:</span> <span class="string">"true"</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      serviceAccountName:</span> <span class="string">filebeat</span></span><br><span class="line"><span class="attr">      terminationGracePeriodSeconds:</span> <span class="number">30</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">filebeat</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">hub.example.io/common/filebeat:6.2.2</span></span><br><span class="line"><span class="attr">        args:</span> <span class="string">[</span></span><br><span class="line">          <span class="string">"-c"</span><span class="string">,</span> <span class="string">"/etc/filebeat.yml"</span><span class="string">,</span></span><br><span class="line">          <span class="string">"-e"</span><span class="string">,</span></span><br><span class="line">        <span class="string">]</span></span><br><span class="line"><span class="attr">        env:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">FILEBEAT_OUTPUT</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">"kafka.example.io:9092"</span></span><br><span class="line"><span class="attr">        securityContext:</span></span><br><span class="line"><span class="attr">          runAsUser:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">        resources:</span></span><br><span class="line"><span class="attr">          limits:</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">200</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">          requests:</span></span><br><span class="line"><span class="attr">            cpu:</span> <span class="number">100</span><span class="string">m</span></span><br><span class="line"><span class="attr">            memory:</span> <span class="number">100</span><span class="string">Mi</span></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">config</span></span><br><span class="line"><span class="attr">          mountPath:</span> <span class="string">/etc/filebeat.yml</span></span><br><span class="line"><span class="attr">          readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">          subPath:</span> <span class="string">filebeat.yml</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">prospectors</span></span><br><span class="line"><span class="attr">          mountPath:</span> <span class="string">/usr/share/filebeat/prospectors.d</span></span><br><span class="line"><span class="attr">          readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">data</span></span><br><span class="line"><span class="attr">          mountPath:</span> <span class="string">/usr/share/filebeat/data</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">varlibdockercontainers</span></span><br><span class="line"><span class="attr">          mountPath:</span> <span class="string">/var/lib/docker/containers</span></span><br><span class="line"><span class="attr">          readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">datalogs</span></span><br><span class="line"><span class="attr">          mountPath:</span> <span class="string">/logs</span></span><br><span class="line"><span class="attr">          readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">config</span></span><br><span class="line"><span class="attr">        configMap:</span></span><br><span class="line"><span class="attr">          defaultMode:</span> <span class="number">0600</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">filebeat-config</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">varlibdockercontainers</span></span><br><span class="line"><span class="attr">        hostPath:</span></span><br><span class="line"><span class="attr">          path:</span> <span class="string">/data/docker/containers</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">datalogs</span></span><br><span class="line"><span class="attr">        hostPath:</span></span><br><span class="line"><span class="attr">          path:</span> <span class="string">/data/logs</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">prospectors</span></span><br><span class="line"><span class="attr">        configMap:</span></span><br><span class="line"><span class="attr">          defaultMode:</span> <span class="number">0600</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">filebeat-prospectors</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">data</span></span><br><span class="line"><span class="attr">        hostPath:</span></span><br><span class="line"><span class="attr">          path:</span> <span class="string">/data/filebeat</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">filebeat</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">- kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">filebeat</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">filebeat</span></span><br><span class="line"><span class="attr">  apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">filebeat</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> <span class="string">filebeat</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="attr">- apiGroups:</span> <span class="string">[""]</span> <span class="comment"># "" indicates the core API group</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">namespaces</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">pods</span></span><br><span class="line"><span class="attr">  verbs:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">get</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">watch</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">list</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">filebeat</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> <span class="string">filebeat</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></table></figure><p>应用接入配置日志写入到<code>/data/logcenter/</code>目录（使用helm）</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">logfile:</span></span><br><span class="line"><span class="attr">  persistent:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  src:</span> <span class="string">/data/logs</span></span><br><span class="line"><span class="attr">  dest:</span> <span class="string">/data/logcenter</span></span><br></pre></td></tr></table></figure><p>app-deployment部分的修改</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - volumeMounts:</span></span><br><span class="line">            <span class="string">&#123;&#123;-</span> <span class="string">if</span> <span class="string">.Values.image.logfile.persistent</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">logfile</span></span><br><span class="line"><span class="attr">              mountPath:</span> <span class="string">&#123;&#123;</span> <span class="string">.Values.image.logfile.dest</span> <span class="string">&#125;&#125;</span></span><br><span class="line">            <span class="string">&#123;&#123;-</span> <span class="string">end</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line">        <span class="string">&#123;&#123;-</span> <span class="string">if</span> <span class="string">.Values.image.logfile.persistent</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">logfile</span></span><br><span class="line"><span class="attr">          hostPath:</span></span><br><span class="line"><span class="attr">            path:</span> <span class="string">&#123;&#123;</span> <span class="string">.Values.image.logfile.src</span> <span class="string">&#125;&#125;</span></span><br><span class="line">        <span class="string">&#123;&#123;-</span> <span class="string">end</span> <span class="string">&#125;&#125;</span></span><br></pre></td></tr></table></figure><p>logstash indexer端以不同的<code>log_topic</code>从消息队列取出处理。</p>]]></content>
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> logging </tag>
            
            <tag> elk </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>挂载pvc的deploy如何rollingupdate</title>
      <link href="/2018/03/31/%E6%8C%82%E8%BD%BDpvc%E7%9A%84deploy%E5%A6%82%E4%BD%95rollingupdate/"/>
      <url>/2018/03/31/%E6%8C%82%E8%BD%BDpvc%E7%9A%84deploy%E5%A6%82%E4%BD%95rollingupdate/</url>
      <content type="html"><![CDATA[<p>假如我们的服务需要保持有状态，通常情况下我们会给pod挂载一个pvc，保证pod-xx被杀掉之后pod-xy起来后仍能访问之前写入文件路径的内容。在使用rollingUpdate的时候，默认的spec.strategy为</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">strategy:</span></span><br><span class="line"><span class="attr">  rollingUpdate:</span></span><br><span class="line"><span class="attr">    maxSurge:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">    maxUnavailable:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">RollingUpdate</span></span><br></pre></td></tr></table></figure><p>这样表示在rolling update过程中，这里假设replicas=1，它需要最少一个pod是<code>status: ready</code>的状态，这个pod通常是旧的deployment产生的pod；同时需要去生成一个新的pod，当新的pod调度到不同的node节点，因为旧的pod仍挂在着pvc，而这类pvc不支持<code>readWriteMany</code>，所以会导致新的pod挂载不上原先的disk，<code>kubectl describe pod</code>显示报错类似<code>Volume is already exclusively attached to one node and can&#39;t be attached to another</code></p><p>解决方法：</p><ul><li><code>.spec.strategy.type==Recreate</code>这个方法比较简单粗暴，在新的pods起来之前会杀掉所有旧的pods</li><li>设置maxUnavailable=0</li></ul><p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy" target="_blank" rel="noopener">deployment/#strategy</a></p>]]></content>
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>使用代理机器缓存gcr的镜像</title>
      <link href="/2018/03/05/%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86%E6%9C%BA%E5%99%A8%E7%BC%93%E5%AD%98gcr%E7%9A%84%E9%95%9C%E5%83%8F/"/>
      <url>/2018/03/05/%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86%E6%9C%BA%E5%99%A8%E7%BC%93%E5%AD%98gcr%E7%9A%84%E9%95%9C%E5%83%8F/</url>
      <content type="html"><![CDATA[<p>因为伟大的gfw的问题，每次下载镜像都需要在有开启了<code>HTTP(S)_PROXY</code>的docker主机上pull下来，save之后复制到目标机器再load到node节点本地。<br>那么我们现在可以通过registry的proxy和dns欺骗的方式来实现只需要一台机器来代理并缓存gcr.io的镜像的功能。</p><ul><li><del>在代理机器的docker配置<code>HTTP(S)_PROXY</code>，保证可以拉取到<strong>外网</strong>的镜像</del>（这一步应该不是必须的）</li><li>运行registry的服务</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -d -p 80:5000 --restart=always -e HTTPS_PROXY=<span class="variable">$proxy_addr</span>:<span class="variable">$port</span> --name registry -v /data/registry:/var/lib/registry -v <span class="built_in">pwd</span>/config.yml:/etc/docker/registry/config.yml registry:2</span><br></pre></td></tr></table></figure><p><code>$proxy_addr:$port</code>可以是同个内网网络中的ss客户端启用了<code>允许来自局域网的连接</code>功能，那么这个proxy_addr就是<code>http://$proxy_addr:$port</code>的格式，走http代理，当然也可以是socks5的代理。</p><p><code>config.yml</code>的配置内容仅是在默认的配置上添加了<code>proxy</code>段</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="number">0.1</span></span><br><span class="line"><span class="attr">log:</span></span><br><span class="line"><span class="attr">  fields:</span></span><br><span class="line"><span class="attr">    service:</span> <span class="string">registry</span></span><br><span class="line"><span class="attr">storage:</span></span><br><span class="line"><span class="attr">  cache:</span></span><br><span class="line"><span class="attr">    blobdescriptor:</span> <span class="string">inmemory</span></span><br><span class="line"><span class="attr">  filesystem:</span></span><br><span class="line"><span class="attr">    rootdirectory:</span> <span class="string">/var/lib/registry</span></span><br><span class="line"><span class="attr">http:</span></span><br><span class="line"><span class="attr">  addr:</span> <span class="string">:5000</span></span><br><span class="line"><span class="attr">  headers:</span></span><br><span class="line"><span class="attr">    X-Content-Type-Options:</span> <span class="string">[nosniff]</span></span><br><span class="line"><span class="attr">health:</span></span><br><span class="line"><span class="attr">  storagedriver:</span></span><br><span class="line"><span class="attr">    enabled:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    interval:</span> <span class="number">10</span><span class="string">s</span></span><br><span class="line"><span class="attr">    threshold:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">proxy:</span></span><br><span class="line"><span class="attr">  remoteurl:</span> <span class="attr">https://gcr.io</span></span><br></pre></td></tr></table></figure><p>然后把客户机的hosts文件中添加记录，将gcr.io指向到代理机的IP地址</p><p>最后一步添加配置到docker的配置文件<code>daemon.json</code></p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"registry-mirrors"</span>: [<span class="string">"http://gcr.io"</span>], </span><br><span class="line">  <span class="attr">"insecure-registries"</span>: [<span class="string">"http://gcr.io"</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>因为代理机上面的registry没有使用tls，所以需要添加insecure的选项。<br>接下来你就可以很愉快地pull外网镜像。</p>]]></content>
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gfw </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>K8s针对资源紧缺处理</title>
      <link href="/2018/02/23/K8s%E9%92%88%E5%AF%B9%E8%B5%84%E6%BA%90%E7%B4%A7%E7%BC%BA%E5%A4%84%E7%90%86/"/>
      <url>/2018/02/23/K8s%E9%92%88%E5%AF%B9%E8%B5%84%E6%BA%90%E7%B4%A7%E7%BC%BA%E5%A4%84%E7%90%86/</url>
      <content type="html"><![CDATA[<p>当前部分node节点可用内存经常低至2Gi，导致prometheus频繁报警。</p><p>调整了告警的规则</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">- alert:</span> <span class="string">NodeMemRunningOut</span></span><br><span class="line"><span class="attr">  expr:</span> <span class="string">node_memory_MemAvailable/1024/1024/1024</span> <span class="string">&lt;=</span> <span class="number">2</span></span><br><span class="line"><span class="attr">  for:</span> <span class="number">10</span><span class="string">m</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    severity:</span> <span class="string">critical</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line"><span class="attr">    description:</span> <span class="string">"节点 <span class="template-variable">&#123;&#123;$labels.instance&#125;&#125;</span> 可用内存将耗尽, 剩余比例 <span class="template-variable">&#123;&#123; $value &#125;&#125;</span>"</span></span><br></pre></td></tr></table></figure><p>再设置kubelet的启动项，添加如下</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">--eviction-hard=memory.available&lt;1.5Gi,nodefs.available&lt;10%,nodefs.inodesFree&lt;5% --system-reserved=memory=3Gi</span><br></pre></td></tr></table></figure><p>当可用内存小于1.5Gi的时候，强制杀死该node节点上的pod，重新分配到其他节点。</p><p>系统服务所需使用最下内存为3Gi。</p><p>重启kubelet服务。</p><p>其实还是要规划计算好单个container的request/limit的值</p><p><a href="https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/</a></p>]]></content>
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> resource </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Kubelet Volume State Metrics</title>
      <link href="/2018/02/09/Kubelet-Volume-State-Metrics/"/>
      <url>/2018/02/09/Kubelet-Volume-State-Metrics/</url>
      <content type="html"><![CDATA[<p>1.9.x版本存在这个问题<br>promethues监控pvc容量遇到的问题</p><ul><li><p>go/src/k8s.io/kubernetes/pkg/kubelet/metrics/metrics.go</p><p>| L206    func Register(containerCache kubecontainer.RuntimeCache)</p></li><li><p>go/src/k8s.io/kubernetes/pkg/kubelet/kubelet.go</p><p>| L591    klet.resourceAnalyzer = serverstats.NewResourceAnalyzer(klet, kubeCfg.VolumeStatsAggPeriod.Duration)</p><p>| L1317    kl.resourceAnalyzer.Start()</p></li><li><p>go/src/k8s.io/kubernetes/pkg/kubelet/server/stats/resource/_analyzer.go</p><p>| L40/L47    func NewResourceAnalyzer(statsProvider StatsProvider, calVolumeFrequency time.Duration) ResourceAnalyzer.Start()</p></li><li><p>go/src/k8s.io/kubernetes/pkg/kubelet/server/stats/fs/_resource/_analyzer.go</p><p>| L60    func (s *fsResourceAnalyzer) Start()</p><p>| L72    func (s *fsResourceAnalyzer) updateCachedPodVolumeStats()</p><p>| L88        entry.StopOnce()</p></li><li><p>go/src/k8s.io/kubernetes/pkg/kubelet/server/stats/volume/_stat/_calculator.go</p><p>| L63    func (s <em>volumeStatCalculator) StartOnce() </em>volumeStatCalculator -&gt; wait.JitterUntil持续调用函数calcAndStoreStats，当stopChannel关闭时停止调用</p><p>| L74    func (s <em>volumeStatCalculator) StopOnce() </em>volumeStatCalculator  -&gt; 关闭了stopChannel</p></li></ul><p>此时仅停止已经删除的pods对应的volumeStatCalculator<br>即注册到prometheus metrics针对这个POD的PVC的指标不会再更新，但是仍然存在，这样exporter去采集的时候仍然采集到这个metric并入库到prometheus中。</p><p>解决方法：</p><ul><li>当出现PVC容量告警时手动重启对应节点的kubelet？</li><li>feature request：在entry.StopOnce()的同时重新Register关于VolumeStats的metrics，即调用unregister再register，<del>或者是不使用定时计算并缓存的方法而采用当prometheus去exporter采集时更新的方式？</del></li><li>忽略这种问题，在prometheus的rule中修改expr规则？</li></ul><p>–volume-stats-agg-period duration Specifies interval for kubelet to calculate and cache the volume disk usage for all pods and volumes.(default 1m0s)</p><p><a href="https://github.com/kubernetes/kubernetes/issues/57686" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/issues/57686</a></p>]]></content>
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> prometheus </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Harbor升级遇到的问题</title>
      <link href="/2018/01/30/Harbor%E5%8D%87%E7%BA%A7%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
      <url>/2018/01/30/Harbor%E5%8D%87%E7%BA%A7%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</url>
      <content type="html"><![CDATA[<h2 id="停止或移除Harbor实例"><a href="#停止或移除Harbor实例" class="headerlink" title="停止或移除Harbor实例"></a>停止或移除Harbor实例</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker-compose down</span><br></pre></td></tr></table></figure><h2 id="备份数据"><a href="#备份数据" class="headerlink" title="备份数据"></a>备份数据</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -ti --rm -e DB_USR=root -e DB_PWD=xxxx -v /data/database:/var/lib/mysql -v /path/to/backup:/harbor-migration/backup vmware/harbor-db-migrator:[tag] backup</span><br></pre></td></tr></table></figure><h2 id="删除或者移动原数据库目录"><a href="#删除或者移动原数据库目录" class="headerlink" title="删除或者移动原数据库目录"></a>删除或者移动原数据库目录</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rm -rf /data/database</span><br></pre></td></tr></table></figure><h2 id="新建Harbor实例，完成数据库初始化"><a href="#新建Harbor实例，完成数据库初始化" class="headerlink" title="新建Harbor实例，完成数据库初始化"></a>新建Harbor实例，完成数据库初始化</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker-compose up -d &amp;&amp; docker-compose stop</span><br></pre></td></tr></table></figure><h2 id="从备份-path-to-backup中恢复数据"><a href="#从备份-path-to-backup中恢复数据" class="headerlink" title="从备份/path/to/backup中恢复数据"></a>从备份/path/to/backup中恢复数据</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -ti --rm -e DB_USR=root -e DB_PWD=xxxx -v /data/database:/var/lib/mysql -v /path/to/backup:/harbor-migration/backup vmware/harbor-db-migrator:[tag] restore</span><br></pre></td></tr></table></figure><h2 id="更新数据库表并迁移数据-optional"><a href="#更新数据库表并迁移数据-optional" class="headerlink" title="更新数据库表并迁移数据(optional)"></a>更新数据库表并迁移数据(optional)</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -ti --rm -e DB_USR=root -e DB_PWD=xxxx -v /data/database:/var/lib/mysql vmware/harbor-db-migrator:[tag] up head</span><br></pre></td></tr></table></figure><h2 id="启动Harbor实例"><a href="#启动Harbor实例" class="headerlink" title="启动Harbor实例"></a>启动Harbor实例</h2><hr><h1 id="在Harbor中删除镜像"><a href="#在Harbor中删除镜像" class="headerlink" title="在Harbor中删除镜像"></a>在Harbor中删除镜像</h1><h2 id="在webUI界面点击删除"><a href="#在webUI界面点击删除" class="headerlink" title="在webUI界面点击删除"></a>在webUI界面点击删除</h2><h2 id="在console执行命令，停止Harbor"><a href="#在console执行命令，停止Harbor" class="headerlink" title="在console执行命令，停止Harbor"></a>在console执行命令，停止Harbor</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker-compose stop</span><br><span class="line">docker run -it --name gc --rm --volumes-from registry vmware/registry:2.6.2-photon garbage-collect --dry-run /etc/registry/config.yml</span><br></pre></td></tr></table></figure><p><strong>Note</strong> the above option <code>--dry-run</code> will print the progress without removeing any data.</p><p>let’s actually do the “cleaning” job</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -it --name gc --rm --volumes-from registry vmware/registry:2.6.2-photon garbage-collect  /etc/registry/config.yml</span><br></pre></td></tr></table></figure><h2 id="启动Harbor"><a href="#启动Harbor" class="headerlink" title="启动Harbor"></a>启动Harbor</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker-compose start</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> harbor </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>使用ceph-exporter监控ceph集群</title>
      <link href="/2018/01/29/%E4%BD%BF%E7%94%A8ceph-exporter%E7%9B%91%E6%8E%A7ceph%E9%9B%86%E7%BE%A4/"/>
      <url>/2018/01/29/%E4%BD%BF%E7%94%A8ceph-exporter%E7%9B%91%E6%8E%A7ceph%E9%9B%86%E7%BE%A4/</url>
      <content type="html"><![CDATA[<h1 id="ceph-exporter"><a href="#ceph-exporter" class="headerlink" title="ceph-exporter"></a><a href="https://github.com/digitalocean/ceph_exporter" target="_blank" rel="noopener">ceph-exporter</a></h1><p>docker version &lt;= 1.13的话不支持build git仓库内的Dockerfile？</p><p>在docker hub下载的镜像太大，所以自己编译打包了一个image</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">go get -u -v github.com/digitalocean/ceph_exporter</span><br><span class="line">mkdir -p ~/ceph_exporter &amp;&amp; <span class="built_in">cd</span> ~/ceph_exporter</span><br><span class="line">cp <span class="variable">$GOPATH</span>/bin/ceph_exporter .</span><br><span class="line"></span><br><span class="line">docker build -t hub.example.io/common/ceph_exporter:2.0.0 .</span><br><span class="line">docker push hub.example.io/common/ceph_exporter:2.0.0</span><br></pre></td></tr></table></figure><h2 id="dockerfile"><a href="#dockerfile" class="headerlink" title="dockerfile"></a>dockerfile</h2><figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="keyword">FROM</span> ubuntu:<span class="number">16.04</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apt-get update \</span></span><br><span class="line"><span class="bash">    &amp;&amp; apt-get install -y librados-dev librbd1 \</span></span><br><span class="line"><span class="bash">    &amp;&amp; rm -rf /var/lib/apt/lists/*</span></span><br><span class="line"><span class="bash"></span></span><br><span class="line"><span class="bash">COPY ceph_exporter /bin/ceph_exporter</span></span><br><span class="line"><span class="bash"></span></span><br><span class="line"><span class="bash">EXPOSE 9128</span></span><br><span class="line"><span class="bash">ENTRYPOINT [<span class="string">"/bin/ceph_exporter"</span>]</span></span><br></pre></td></tr></table></figure><h2 id="deployment-svc"><a href="#deployment-svc" class="headerlink" title="deployment/svc"></a>deployment/svc</h2><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ceph-exporter</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">monitoring</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">ceph-exporter</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">ceph-exporter</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">ceph-exporter</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">hub.example.io/common/ceph_exporter:2.0.0</span></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">http-metrics</span></span><br><span class="line"><span class="attr">          containerPort:</span> <span class="number">9128</span></span><br><span class="line"><span class="attr">          protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - mountPath:</span> <span class="string">/etc/localtime</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">localtime</span></span><br><span class="line"><span class="attr">          readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">        - mountPath:</span> <span class="string">/etc/ceph</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">ceph-cfg</span></span><br><span class="line"><span class="attr">          readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">      hostAliases:</span></span><br><span class="line"><span class="attr">      - ip:</span> <span class="string">"172.20.10.91"</span></span><br><span class="line"><span class="attr">        hostnames:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"node01"</span></span><br><span class="line"><span class="attr">      - ip:</span> <span class="string">"172.20.10.97"</span></span><br><span class="line"><span class="attr">        hostnames:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"node06"</span></span><br><span class="line"><span class="attr">      - ip:</span> <span class="string">"172.20.10.98"</span></span><br><span class="line"><span class="attr">        hostnames:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"node07"</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">localtime</span></span><br><span class="line"><span class="attr">        hostPath:</span></span><br><span class="line"><span class="attr">          path:</span> <span class="string">/etc/localtime</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">ceph-cfg</span></span><br><span class="line"><span class="attr">        hostPath:</span></span><br><span class="line"><span class="attr">          path:</span> <span class="string">/etc/ceph</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ceph-exporter</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">monitoring</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> <span class="string">ceph-exporter</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">ceph-exporter</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">http-metrics</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">9128</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">9128</span></span><br></pre></td></tr></table></figure><p>添加<code>hostAliases</code>是因为<code>ceph.conf</code>中mon是用的hostname的方式，不添加到container的host记录的话会解析不到这些节点的IP</p><h2 id="重点，利用prometheus-operator的servicemonitor"><a href="#重点，利用prometheus-operator的servicemonitor" class="headerlink" title="重点，利用prometheus-operator的servicemonitor"></a>重点，利用prometheus-operator的<code>servicemonitor</code></h2><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">monitoring.coreos.com/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceMonitor</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ceph-exporter</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    k8s-app:</span> <span class="string">ceph-exporter</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  jobLabel:</span> <span class="string">k8s-app</span></span><br><span class="line"><span class="attr">  endpoints:</span></span><br><span class="line"><span class="attr">  - port:</span> <span class="string">http-metrics</span></span><br><span class="line"><span class="attr">    interval:</span> <span class="number">30</span><span class="string">s</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      k8s-app:</span> <span class="string">ceph-exporter</span></span><br><span class="line"><span class="attr">  namespaceSelector:</span></span><br><span class="line"><span class="attr">    matchNames:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">monitoring</span></span><br></pre></td></tr></table></figure><p>新建serviceMonitor</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl create -n monitoring -f ceph_exporter-sm.yaml</span><br></pre></td></tr></table></figure><p>过一会我们查看prometheus的日志可以看到自动reload了配置，config中已经有了ceph-exporter这个job，并且可以查询到ceph-exportor检测到的metrics了。</p><p>优化项</p><ul><li>添加nodeSelector，因为部分节点假如不在ceph-admin节点执行<code>ceph-deploy admin nodename</code>的话，<code>/etc/ceph/</code>目录并不存在，exporter依赖于该目录下的<code>ceph.client.admin.keyring</code>和<code>ceph.conf</code></li></ul><p><a href="https://grafana.com/dashboards/917" target="_blank" rel="noopener">grafana-dashboard</a></p>]]></content>
      
      
    </entry>
    
    <entry>
      <title>Prometheus Operator</title>
      <link href="/2018/01/26/Prometheus-Operator/"/>
      <url>/2018/01/26/Prometheus-Operator/</url>
      <content type="html"><![CDATA[<h3 id="prometheus-operator"><a href="#prometheus-operator" class="headerlink" title="prometheus-operator"></a>prometheus-operator</h3><blockquote><p>promethues-operator是一个通过监听K8s内CRD资源的变更操作事件来自动创建，配置并管理prometheus监控系统的一个控制器，可以理解成是一个类似于controller-manager的东西，只不过它的管理对象不是ds/deploy/sts/svc等</p></blockquote><p><a href="https://coreos.com/operators/prometheus/docs/latest/" target="_blank" rel="noopener">官网</a>/<a href="https://github.com/coreos/prometheus-operator" target="_blank" rel="noopener">github</a></p><hr><p>整套方案包括以下组件</p><ul><li><p><a href="https://prometheus.io" target="_blank" rel="noopener">prometheus</a></p><p>prometheus是一个开源的监控告警系统，具有由度量名称和键/值对标识的时间序列数据的多维数据模型、灵活的查询语言，监控模式是通过HTTP主动去拉取exporters上基于时间序列的监控采集数据，同时也能通过中间网关来支持推送型的监控数据收集，所有的监控目标都是通过配置型的或是服务发现，或是静态配置，提供了HTTP页面支持图形和仪表盘的展示。</p><p>  <img src="https://prometheus.io/assets/architecture.svg" alt=""></p></li></ul><ul><li><p><a href="https://prometheus.io/docs/alerting/alertmanager/" target="_blank" rel="noopener">alertmanager</a></p><p>alertmanager处理由客户端应用程序发送的prometheus警报，将重复的告警信息通过特定的标签去分组，并将它们路由到正确的接收方，特点是能够grouping、抑制、设置静默等。</p></li><li><p><a href="https://github.com/fengxsong/dingtalk" target="_blank" rel="noopener">一个简单的钉钉机器人</a></p><p>从alertmanager通过webhook的方式发送告警信息，将payload渲染成配置好的模板，并发送到指定的钉钉机器人</p></li><li><p><a href="https://grafana.com/" target="_blank" rel="noopener">grafana</a></p><p>有了prometheus去收集数据，那我们还需要将这些metrics通过web页面展示出来，那grafana就是这样的一个工具，它支持多种数据源，能让你查询、展示或者基于这些告警数据来发送警报（虽然也支持prometheus数据源但是我们并没有使用到），最实用的是它在社区有维护了很多开箱即用的面板模板，只需要少少的修改就能展示出一个很详细的展示面板出来。当然，它也实现了自己的一套DSL语言。</p></li><li><p><a href="https://github.com/prometheus/node_exporter" target="_blank" rel="noopener">node-exporter</a></p><p>一个golang写的能够采集硬件以及OS信息的收集器，采集目标包括cpu/硬盘/conntrack/文件系统/负载/网络连接信息/硬件信息等，在K8S内使用daemonSet的方式运行，保证每台节点主机都能监控起自身的信息</p></li><li><p><a href="https://github.com/google/cadvisor" target="_blank" rel="noopener">cadvisor</a></p><p>示例中的prometheus-operator生成的监控目标包含了cadvisor<br>虽然K8S组件kubelet编译包含了cadvisor，但是默认是不启用的。<br>cadvisor也是通过HTTP的方式暴露了节点主机上的资源使用率、性能指标以及运行的容器信息</p></li><li><p><a href="https://github.com/kubernetes/kube-state-metrics" target="_blank" rel="noopener">kube-state-metrics</a></p><p>这个插件通过去APISERVER获取K8S内对象并生成对象对应的监控数据，例如nodes、pods、deployments等，具体可查阅[文档][10]</p></li><li><p>当然还有各个组件自带的metrics</p></li></ul><p>其实倒数四个都是属于采集器，负责收集对应系统组件的信息。</p><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https://github.com/coreos/prometheus-operator</span><br><span class="line">cd prometheus-operator/contrib/kube-prometheus/</span><br><span class="line">bash hack/cluster-monitoring/deploy</span><br></pre></td></tr></table></figure><h3 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h3><p>prometheus-operator使用了k8s1.8+引入的CRD(custom resource definitions)，实现了controller的功能，通俗点来说，它负责将resource definition转换成K8S里的statefulset或者是promethues的配置对象。</p><ul><li><p><a href="https://github.com/coreos/prometheus-operator/blob/master/example/prometheus-operator-crd/prometheus.crd.yaml" target="_blank" rel="noopener">Prometheus</a></p><p><code>Operator</code>会<code>ListWatch</code>集群内的<code>Prometheus CRD</code>来创建一个合适的<code>statefulset</code>在<code>monitoring</code>(.metadata.namespace指定)命名空间，并且挂载了一个名为<code>prometheus-k8s</code>的<code>Secret</code>为Volume到<code>/etc/prometheus/config</code>目录，<code>Secret</code>的data包含了以下内容</p><ul><li>configmaps.json指定了rule-files在configmap的名字</li><li>prometheus.yaml为主配置文件</li></ul></li><li><p><a href="https://github.com/coreos/prometheus-operator/blob/master/example/prometheus-operator-crd/servicemonitor.crd.yaml" target="_blank" rel="noopener">ServiceMonitor</a></p><p>这个允许动态地监听K8S里的<code>Service</code>，会将生成的job更新到上面的<code>prometheus-k8s</code>这个<code>Secret</code>的 <code>Data.**prometheus.yaml**</code>里，然后prometheus这个pod里的sidecar容器<code>prometheus-config-reloader</code>当检测到挂载路径的文件发生改变后自动去执行HTTPPost请求到<code>/api/-reload-</code>路径去reload配置</p></li><li><p><a href="https://github.com/coreos/prometheus-operator/blob/master/example/prometheus-operator-crd/alertmanager.crd.yaml" target="_blank" rel="noopener">Alertmanager</a></p><p>这个将生成一个<code>statefulset</code>类型对象，并挂载了名为<code>alertmanager-main</code>的<code>secret</code>资源到容器内部的<code>/etc/alertmanager/config/alertmanager.yaml</code>路径。当需要更新配置文件时需要将<code>alertmanager.yaml</code>内容base64encode后更新到<code>alertmanager-main</code>的<code>Data</code>属性或者直接patch secret from file</p></li></ul><h3 id="比较疑惑的地方"><a href="#比较疑惑的地方" class="headerlink" title="比较疑惑的地方"></a>比较疑惑的地方</h3><p>默认的<code>namespace</code>是<code>monitoring</code>，假如我把deployment/svc/servicemonitor都创建在其他命名空间例如<code>default</code>，operator并不会将这个servicemonitor配置到prometheus.yaml中，有两个原因导致</p><ol><li>由于<code>Prometheus</code>object的配置</li></ol><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">serviceMonitorSelector:</span></span><br><span class="line"><span class="attr">  matchExpressions:</span></span><br><span class="line"><span class="attr">  - key:</span> <span class="string">k8s-app</span></span><br><span class="line"><span class="attr">    operator:</span> <span class="string">Exists</span></span><br></pre></td></tr></table></figure><p>假如servicemonitor(以下简写sm)的<code>.metadata.labels</code>不包含<code>k8s-app</code>这个label的话会被忽略，处理方法修改sm添加label以及添加prometheus的serviceMonitorSelector</p><ol start="2"><li><code>createConfig</code>函数中首先用LabelSelector去List出匹配的sm，具体逻辑在<code>selectServiceMonitors</code>，当<code>ServiceMonitorNamespaceSelector</code>为nil或Size为0时仅查找当前namespace。<br>如果想List所有ns的sm，则简单添加<a href="https://github.com/coreos/prometheus-operator/blob/master/example/prometheus-operator-crd/prometheus.crd.yaml#L2001" target="_blank" rel="noopener">serviceMonitorNamespaceSelector: {}</a></li></ol>]]></content>
      
      <categories>
          
          <category> prometheus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> prometheus </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Expanding PVC</title>
      <link href="/2018/01/18/Expanding-PVC/"/>
      <url>/2018/01/18/Expanding-PVC/</url>
      <content type="html"><![CDATA[<ul><li><a href="https://kubernetes.io/docs/admin/admission-controllers/#persistentvolumeclaimresize" target="_blank" rel="noopener">admission-controller开启PersistentVolumeClaimsResize</a></li><li><a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims" target="_blank" rel="noopener">expanding Persistent Volume Claims</a></li></ul>]]></content>
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> pvc </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pod卡在terminated状态</title>
      <link href="/2017/12/25/Pod%E5%8D%A1%E5%9C%A8terminated%E7%8A%B6%E6%80%81/"/>
      <url>/2017/12/25/Pod%E5%8D%A1%E5%9C%A8terminated%E7%8A%B6%E6%80%81/</url>
      <content type="html"><![CDATA[<p>在部署helm的过程中，因为大中华防火墙的问题导致了一台node节点拉不到容器，之后所有内部仓库的镜像也拉取不到？</p><p>delete pod并不能删除pod，那么我们需要强制删除到</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl delete pod NAME --grace-period=0 --force</span><br></pre></td></tr></table></figure><p>从kubelet跟docker的日志完全看不出问题，之后使用重启大法重启了kubelet就恢复了(这做法太蠢了)</p>]]></content>
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>StorageClass的ReclaimPolicy</title>
      <link href="/2017/11/28/StorageClass%E7%9A%84ReclaimPolicy/"/>
      <url>/2017/11/28/StorageClass%E7%9A%84ReclaimPolicy/</url>
      <content type="html"><![CDATA[<h4 id="reclaimPolicy-Retain"><a href="#reclaimPolicy-Retain" class="headerlink" title="reclaimPolicy: Retain"></a><code>reclaimPolicy: Retain</code></h4><p>默认为<code>DELETE</code>，会导致当删除PVC时，由storageclass自动生成的PV也会跟着删除，而这个PV可能已经保存了用户不需要删除的数据。所以设置<code>reclaimPolicy: Retain</code>，但是好像没有生效？</p><h4 id="edit-manually"><a href="#edit-manually" class="headerlink" title="edit manually"></a>edit manually</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get pv</span><br><span class="line">kubectl patch pv &lt;your-pv-name&gt; -p &apos;&#123;&quot;spec&quot;:&#123;&quot;persistentVolumeReclaimPolicy&quot;:&quot;Retain&quot;&#125;&#125;&apos;</span><br></pre></td></tr></table></figure><h4 id="change-default-storageclass"><a href="#change-default-storageclass" class="headerlink" title="change default storageclass"></a>change default storageclass</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl patch storageclass rbd -p &apos;&#123;&quot;metadata&quot;: &#123;&quot;annotations&quot;:&#123;&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;false&quot;&#125;&#125;&#125;&apos;</span><br></pre></td></tr></table></figure><p>解决storageclass的reclaimPolicy的问题（现在应该不存在这问题了）</p><p>最新的release的代码中<a href="https://github.com/kubernetes-incubator/external-storage/blob/master/ceph/rbd/pkg/provision/provision.go#L125" target="_blank" rel="noopener">provision.go</a>已经适配了k8s-1.8的api接口，支持了<code>PersistentVolumeReclaimPolicy</code></p><p>但是<code>quay.io/external_storage/rbd-provisioner:latest</code> 这个镜像并没有更新推上去…</p><p>手动编译打包并推到内部registry，然后修改deployment的image为内部仓库镜像即可</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">go get -u github.com/kubernetes-incubator/external-storage</span><br><span class="line"><span class="built_in">cd</span> ~/go/src/github.com/kubernetes-incubator/external-storage/ceph/rbd</span><br><span class="line">make push</span><br><span class="line"><span class="comment">#因为上面make push生产的tag是latest，所以我们需要手动给打上其他标签</span></span><br><span class="line">docker tag quay.io/external_storage/rbd-provisioner:latest hub.example.io/common/rbd-provisioner:latest</span><br></pre></td></tr></table></figure><hr><h4 id="ReclaimPolicy策略为Retain时回收Release状态的pv"><a href="#ReclaimPolicy策略为Retain时回收Release状态的pv" class="headerlink" title="ReclaimPolicy策略为Retain时回收Release状态的pv"></a>ReclaimPolicy策略为Retain时回收Release状态的pv</h4><p>查看到STATUS为Released的PV，直接删除delete pv的方式，provisioner并不会监听这些事件去删除实际在pool中的pv，所以我们需要给这个pv更新下属性</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubectl get pv</span><br><span class="line">kubectl patch pv pvc-fce10e1d-c9e8-11e7-aa87-005056b12f99 --patch <span class="string">'&#123;"spec": &#123;"persistentVolumeReclaimPolicy": "Delete"&#125;&#125;'</span></span><br></pre></td></tr></table></figure><p>这样子rbd-provisioner会在监听到pv属性变更事件后调用controller.Provisioner.Delete方法</p><p>接下来可以去ceph rbd看到存储已经被删除掉了</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo rbd ls -p kube</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pvc </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Deploy Harbor registry in K8s</title>
      <link href="/2017/11/28/Deploy-Harbor-registry-in-K8s/"/>
      <url>/2017/11/28/Deploy-Harbor-registry-in-K8s/</url>
      <content type="html"><![CDATA[<ul><li>without-tls模式下，<code>daemon.json</code>需要配置<code>insecure-registries</code></li><li>不能使用ingress自动配置域名</li></ul><p>原因是docker push的时候没有添加Host的请求头，导致将请求转发到默认后端导致了404。</p><p>现在是使用<code>ingress-nginx</code> + <code>keepalived</code>的组合，使用了<code>DaemonSet</code>的方式，需要调整成<code>Deployment</code>+<code>nodeSelector</code>或者<code>affinity</code>限定LBS运行在某些node节点，再在其他node节点用<code>hostNetwork</code>的方式来跑harbor的nginx组件。</p>]]></content>
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> harbor </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>K8s的Volume</title>
      <link href="/2017/11/15/K8s%E7%9A%84Volume/"/>
      <url>/2017/11/15/K8s%E7%9A%84Volume/</url>
      <content type="html"><![CDATA[<p>当需要将多个已存在的volume放在同一个目录的时候，使用<code>projected</code>类型</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">demo</span></span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">all-in-one</span></span><br><span class="line"><span class="attr">      mountPath:</span> <span class="string">/projected</span></span><br><span class="line"><span class="attr">      readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">all-in-one</span></span><br><span class="line"><span class="attr">    projected:</span></span><br><span class="line"><span class="attr">      sources:</span></span><br><span class="line"><span class="attr">      - secret:</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">user</span></span><br><span class="line"><span class="attr">      - secret:</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">pass</span></span><br></pre></td></tr></table></figure><h2 id="secret类型"><a href="#secret类型" class="headerlink" title="secret类型"></a>secret类型</h2><h3 id="创建secret"><a href="#创建secret" class="headerlink" title="创建secret"></a>创建secret</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create files containing the username and password:</span></span><br><span class="line"><span class="built_in">echo</span> -n <span class="string">"admin"</span> &gt; ./username.txt</span><br><span class="line"><span class="built_in">echo</span> -n <span class="string">"1f2d1e2e67df"</span> &gt; ./password.txt</span><br><span class="line"><span class="comment"># Package these files into secrets:</span></span><br><span class="line">kubectl create secret generic user --from-file=./username.txt</span><br><span class="line">kubectl create secret generic pass --from-file=./password.txt</span><br></pre></td></tr></table></figure><p><code>emptyDir</code>是表示在这个POD的存活周期内不会消失不见。</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">sec-ctx-vol</span></span><br><span class="line"><span class="attr">    emptyDir:</span> <span class="string">&#123;&#125;</span></span><br></pre></td></tr></table></figure><hr><h2 id="pod跟configmap-cm"><a href="#pod跟configmap-cm" class="headerlink" title="pod跟configmap(cm)"></a>pod跟configmap(cm)</h2><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">kubectl</span> <span class="string">create</span> <span class="string">configmap</span> <span class="string">special-config</span> <span class="bullet">--from-literal=special.how=very</span></span><br><span class="line"><span class="string">$</span> <span class="string">kubectl</span> <span class="string">edit</span> <span class="string">pod</span> <span class="string">dapi-test-pod</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">test-container</span></span><br><span class="line"><span class="attr">    image:</span></span><br><span class="line"><span class="attr">    env:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">TEST_ENV</span></span><br><span class="line"><span class="attr">      valueFrom:</span></span><br><span class="line"><span class="attr">        configMapKeyRef:</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">special-config</span></span><br><span class="line"><span class="attr">          key:</span> <span class="string">special.how</span></span><br></pre></td></tr></table></figure><h3 id="将configMap的data作为env变量"><a href="#将configMap的data作为env变量" class="headerlink" title="将configMap的data作为env变量"></a>将configMap的data作为env变量</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: test-containers</span><br><span class="line">    envFrom:</span><br><span class="line">    - configMapRef:</span><br><span class="line">        name: special-config</span><br></pre></td></tr></table></figure><hr><h2 id="pod跟secret"><a href="#pod跟secret" class="headerlink" title="pod跟secret"></a>pod跟secret</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl create secret generic <span class="built_in">test</span>-secret --from-literal=username=<span class="string">'my-app'</span> --from-literal=password=<span class="string">'39528$vdg7Jb'</span></span><br></pre></td></tr></table></figure><p>pod中将secret以文件方式挂载为volume</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span></span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">secret-volume</span></span><br><span class="line"><span class="attr">      mountPath:</span> <span class="string">/etc/secret-volume</span></span><br><span class="line"><span class="attr">   volume:</span></span><br><span class="line"><span class="attr">   - name:</span> <span class="string">secret-volume</span></span><br><span class="line"><span class="attr">     secret:</span></span><br><span class="line"><span class="attr">        secretName:</span> <span class="string">test-secret</span></span><br></pre></td></tr></table></figure><p>pod中将secret的值作为环境变量</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">xxx</span></span><br><span class="line"><span class="attr">    env:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">SECRET_USERNAME</span></span><br><span class="line"><span class="attr">      valueFrom:</span></span><br><span class="line"><span class="attr">         secretKeyRef:</span></span><br><span class="line"><span class="attr">            key:</span> <span class="string">username</span></span><br><span class="line"><span class="attr">            name:</span> <span class="string">test-secret</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">SECRET_PASSWORD</span></span><br><span class="line"><span class="attr">      valueFrom:</span></span><br><span class="line"><span class="attr">         secretKeyRef:</span></span><br><span class="line"><span class="attr">            key:</span> <span class="string">password</span></span><br><span class="line"><span class="attr">            name:</span> <span class="string">test-secret</span></span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>IngressController与IPVS</title>
      <link href="/2017/11/15/IngressController%E4%B8%8EIPVS/"/>
      <url>/2017/11/15/IngressController%E4%B8%8EIPVS/</url>
      <content type="html"><![CDATA[<h1 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h1><p><strong>April.20 Update:</strong><br>可参考或是直接试用caicloud的开源实现 <a href="https://github.com/caicloud/loadbalancer-controller" target="_blank" rel="noopener">loadbalancer-controller</a></p><p><strong>April.4 Update:</strong></p><p>尝试使用direct route的方式，lvs节点会绑定VIP的地址，通过flannel overlay网络将用户req发送到后端的pod节点，后端的ingress-nginx这个service需添加externalIPs为VIP的地址，之后我们可以看到kube-proxy在nat表添加了对应的iptables规则</p><p>查看规则</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo iptables -t nat -L KUBE-SERVICES -n</span><br></pre></td></tr></table></figure><p>以上步骤与传统的IPVS DR配置模式不同的是并未在ingress-nginx-controller的节点上手动绑定VIP到loopback口，再关闭arp响应操作。</p><p>但是实际在lvs节点抓包显示的情况是这样子的：</p><ul><li>clientIP req -&gt; VIP</li><li>VIP经过NAT将请求从10.244.1.0(cni0)这个口走flannel.1路由到10.244.4.10这个节点的pod</li><li>正常来说10.244.4.10应该是直接经过NAT再返回给client，但是实际上是10.244.4.10返回给10.244.1.0再NAT到VIP，最后返回给了client</li></ul><p>这个难道不就是NAT模式吗…</p><hr><h2 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h2><p>首先需要有一个<code>ingress</code>的服务，因为之前<code>ingress</code>的类型为<code>Deployment</code> ,将泛域名解析到单独一台node节点的话会有单点的问题，需要一点变更</p><h3 id="deployment"><a href="#deployment" class="headerlink" title="deployment"></a>deployment</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">nginx-ingress-controller</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">ingress-nginx</span> </span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">ingress-nginx</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">ingress-nginx</span></span><br><span class="line"><span class="attr">      annotations:</span></span><br><span class="line">        <span class="string">prometheus.io/port:</span> <span class="string">'10254'</span></span><br><span class="line">        <span class="string">prometheus.io/scrape:</span> <span class="string">'true'</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      serviceAccountName:</span> <span class="string">nginx-ingress-serviceaccount</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">nginx-ingress-controller</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.9.0</span></span><br><span class="line"><span class="attr">          args:</span></span><br><span class="line"><span class="bullet">            -</span> <span class="string">/nginx-ingress-controller</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--default-backend-service=$(POD_NAMESPACE)/default-http-backend</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--configmap=$(POD_NAMESPACE)/nginx-configuration</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--tcp-services-configmap=$(POD_NAMESPACE)/tcp-services</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--udp-services-configmap=$(POD_NAMESPACE)/udp-services</span></span><br><span class="line"><span class="bullet">            -</span> <span class="bullet">--annotations-prefix=nginx.ingress.kubernetes.io</span></span><br><span class="line"><span class="attr">          env:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">POD_NAME</span></span><br><span class="line"><span class="attr">              valueFrom:</span></span><br><span class="line"><span class="attr">                fieldRef:</span></span><br><span class="line"><span class="attr">                  fieldPath:</span> <span class="string">metadata.name</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">POD_NAMESPACE</span></span><br><span class="line"><span class="attr">              valueFrom:</span></span><br><span class="line"><span class="attr">                fieldRef:</span></span><br><span class="line"><span class="attr">                  fieldPath:</span> <span class="string">metadata.namespace</span></span><br><span class="line"><span class="attr">          ports:</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">http</span></span><br><span class="line"><span class="attr">            containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">https</span></span><br><span class="line"><span class="attr">            containerPort:</span> <span class="number">443</span></span><br><span class="line"><span class="attr">          livenessProbe:</span></span><br><span class="line"><span class="attr">            failureThreshold:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">            httpGet:</span></span><br><span class="line"><span class="attr">              path:</span> <span class="string">/healthz</span></span><br><span class="line"><span class="attr">              port:</span> <span class="number">10254</span></span><br><span class="line"><span class="attr">              scheme:</span> <span class="string">HTTP</span></span><br><span class="line"><span class="attr">            initialDelaySeconds:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">            periodSeconds:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">            successThreshold:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">            timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">          readinessProbe:</span></span><br><span class="line"><span class="attr">            failureThreshold:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">            httpGet:</span></span><br><span class="line"><span class="attr">              path:</span> <span class="string">/healthz</span></span><br><span class="line"><span class="attr">              port:</span> <span class="number">10254</span></span><br><span class="line"><span class="attr">              scheme:</span> <span class="string">HTTP</span></span><br><span class="line"><span class="attr">            periodSeconds:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">            successThreshold:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">            timeoutSeconds:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">      nodeSelector:</span></span><br><span class="line"><span class="attr">        ingress-instance:</span> <span class="string">"true"</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ingress-nginx</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">ingress-nginx</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">ingress-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">http</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">    protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">https</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">443</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">443</span></span><br><span class="line"><span class="attr">    protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">ingress-nginx</span></span><br></pre></td></tr></table></figure><ul><li>Kind 换成 <code>DaemonSet</code></li><li>去除<code>hostNetwork: true</code></li><li>添加<code>Service</code></li></ul><h3 id="rbac"><a href="#rbac" class="headerlink" title="rbac"></a>rbac</h3><p>基于rbac的认证类型，需要添加<code>sa/clusterrole/clusterrolebinding</code></p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">kube-keepalived-vip</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="attr">- apiGroups:</span> <span class="string">[""]</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">pods</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">nodes</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">endpoints</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">services</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">configmaps</span></span><br><span class="line"><span class="attr">  verbs:</span> <span class="string">["get",</span> <span class="string">"list"</span><span class="string">,</span> <span class="string">"watch"</span><span class="string">]</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">kube-keepalived-vip</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">ingress-nginx</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">kube-keepalived-vip</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">  kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">kube-keepalived-vip</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">- kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">kube-keepalived-vip</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">ingress-nginx</span></span><br></pre></td></tr></table></figure><h3 id="configmap"><a href="#configmap" class="headerlink" title="configmap"></a>configmap</h3><p>新建configmap，供kube-keepalived-vip实例在集群中读取配置生成keepalived的配置</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">keepalived-vip-configmap</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">ingress-nginx</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="number">172.20</span><span class="number">.10</span><span class="number">.94</span><span class="string">:</span> <span class="string">ingress-nginx/ingress-nginx</span></span><br></pre></td></tr></table></figure><p>data的格式为<code>externalVIP: namespace/svc(:DR|NAT)</code>，括号内为<code>lvs_method</code>，不指定默认为<code>NAT</code>，指明<code>DR</code>的话客户端请求回不来。</p><blockquote><p>DR的话需要在RS的节点的lo网卡绑定vip，配置arp的参数，添加到vip的路由。</p></blockquote><h3 id="keepalived-controller"><a href="#keepalived-controller" class="headerlink" title="keepalived controller"></a>keepalived controller</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">kube-keepalived-vip</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">ingress-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        name:</span> <span class="string">kube-keepalived-vip</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      hostNetwork:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">      serviceAccount:</span> <span class="string">kube-keepalived-vip</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">kube-keepalived-vip</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">gcr.io/google-containers/kube-keepalived-vip:0.11</span></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"><span class="attr">        securityContext:</span></span><br><span class="line"><span class="attr">          privileged:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - mountPath:</span> <span class="string">/lib/modules</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">modules</span></span><br><span class="line"><span class="attr">          readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">        - mountPath:</span> <span class="string">/dev</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">dev</span></span><br><span class="line"><span class="attr">        env:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">POD_NAME</span></span><br><span class="line"><span class="attr">          valueFrom:</span></span><br><span class="line"><span class="attr">            fieldRef:</span></span><br><span class="line"><span class="attr">              fieldPath:</span> <span class="string">metadata.name</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">POD_NAMESPACE</span></span><br><span class="line"><span class="attr">          valueFrom:</span></span><br><span class="line"><span class="attr">            fieldRef:</span></span><br><span class="line"><span class="attr">              fieldPath:</span> <span class="string">metadata.namespace</span></span><br><span class="line"><span class="attr">        args:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="bullet">--services-configmap=ingress-nginx/keepalived-vip-configmap</span></span><br><span class="line"><span class="bullet">        -</span> <span class="bullet">-v=9</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">modules</span></span><br><span class="line"><span class="attr">        hostPath:</span></span><br><span class="line"><span class="attr">         path:</span> <span class="string">/lib/modules</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">dev</span></span><br><span class="line"><span class="attr">        hostPath:</span></span><br><span class="line"><span class="attr">         path:</span> <span class="string">/dev</span></span><br><span class="line"><span class="attr">      nodeSelector:</span></span><br><span class="line"><span class="attr">        ingress-instance:</span> <span class="string">"true"</span></span><br></pre></td></tr></table></figure><p>注意以下没配置正确的话启动会报错</p><ul><li><code>spec.template.spec</code>有<code>serviceAccount: kube-keepalived-vip</code>的配置，不然该实例不能监听到K8S集群内的事件</li><li><code>spec.template.spec.containers[].args</code>需指定上面新建的<code>configmap</code></li></ul><p>检查测试的话，可以查看<code>kube-keepalived-vip-*</code>的pod的log或者是生成的keepalived.conf<br>watch pod状态的话一直Error的话，一般都是有error导致退出，查看log就能找到原因了。</p>]]></content>
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> ipvs </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>两种分配Pods的方式</title>
      <link href="/2017/11/13/%E4%B8%A4%E7%A7%8D%E5%88%86%E9%85%8DPods%E7%9A%84%E6%96%B9%E5%BC%8F/"/>
      <url>/2017/11/13/%E4%B8%A4%E7%A7%8D%E5%88%86%E9%85%8DPods%E7%9A%84%E6%96%B9%E5%BC%8F/</url>
      <content type="html"><![CDATA[<h3 id="使用nodeSelector"><a href="#使用nodeSelector" class="headerlink" title="使用nodeSelector"></a>使用nodeSelector</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get nodes --show-labels</span><br></pre></td></tr></table></figure><p>没有满足的labels时我们可以另外给node节点打标签</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl label nodes node02 ingress-instance=<span class="literal">true</span></span><br></pre></td></tr></table></figure><p>删除label</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl label nodes node02 ingress-instance-</span><br></pre></td></tr></table></figure><p>ds/deploy/sts等使用nodeSelector</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line">      <span class="string">...</span></span><br><span class="line"><span class="attr">      nodeSelector:</span></span><br><span class="line"><span class="attr">        ingress-instance:</span> <span class="string">"true"</span></span><br></pre></td></tr></table></figure><h3 id="Affinity-and-anti-affinity"><a href="#Affinity-and-anti-affinity" class="headerlink" title="Affinity and anti-affinity"></a>Affinity and anti-affinity</h3><ul><li>requiredDuringSchedulingIgnoredDuringExecution</li><li>preferredDuringSchedulingIgnoredDuringExecution</li></ul><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  affinity:</span></span><br><span class="line"><span class="attr">    nodeAffinity:</span></span><br><span class="line"><span class="attr">      requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line"><span class="attr">        nodeSelectorTerms:</span></span><br><span class="line"><span class="attr">        - matchExpressions:</span></span><br><span class="line"><span class="attr">          - key:</span> <span class="string">ingress-instance</span></span><br><span class="line"><span class="attr">            operator:</span> <span class="string">In</span></span><br><span class="line"><span class="attr">            values:</span> <span class="string">"true"</span></span><br><span class="line"><span class="attr">      preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">        <span class="string">...</span></span><br></pre></td></tr></table></figure><p><a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity" target="_blank" rel="noopener">REF</a></p>]]></content>
      
      <categories>
          
          <category> k8s </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>K8s使用cephRDB块作为动态存储</title>
      <link href="/2017/10/19/K8s%E4%BD%BF%E7%94%A8cephRDB%E5%9D%97%E4%BD%9C%E4%B8%BA%E5%8A%A8%E6%80%81%E5%AD%98%E5%82%A8/"/>
      <url>/2017/10/19/K8s%E4%BD%BF%E7%94%A8cephRDB%E5%9D%97%E4%BD%9C%E4%B8%BA%E5%8A%A8%E6%80%81%E5%AD%98%E5%82%A8/</url>
      <content type="html"><![CDATA[<h1 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1><h2 id="Install-ceph-common-package"><a href="#Install-ceph-common-package" class="headerlink" title="Install ceph-common package"></a>Install ceph-common package</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get ceph-common</span><br></pre></td></tr></table></figure><h2 id="Create-Pool-for-dynamic-volumes"><a href="#Create-Pool-for-dynamic-volumes" class="headerlink" title="Create Pool for dynamic volumes"></a>Create Pool for dynamic volumes</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ceph osd pool create kube 1024</span><br><span class="line">$ ceph auth get-or-create client.kube mon <span class="string">'allow r'</span> osd <span class="string">'allow class-read object_prefix rbd_children, allow rwx pool=kube'</span> -o ceph.client.kube.keyring</span><br></pre></td></tr></table></figure><p>mon=<code>allow r</code>read mon to find osd<br>osd=<code>allow class-read object_prefix rbd_children, allow rwx pool=kube</code>read rbd_children prefix, full access to kube pool)</p><h2 id="Creating-ceph-secret"><a href="#Creating-ceph-secret" class="headerlink" title="Creating ceph secret"></a>Creating ceph secret</h2><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ceph-secret</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">"kubernetes.io/rbd"</span>  </span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"><span class="attr">  key:</span> <span class="string">QVFCaHplWlorTTJaQ3hBQVJXbHhwTnZXTnpEMXB0V1YzdEJyVHc9PQ==</span></span><br></pre></td></tr></table></figure><p><code>data.key</code> is generated on one of the ceph mon nodes</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph auth get-key client.admin | base64</span><br></pre></td></tr></table></figure><h2 id="Create-ceph-user-secret"><a href="#Create-ceph-user-secret" class="headerlink" title="Create ceph user secret"></a>Create ceph user secret</h2><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ceph-user-secret</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"><span class="attr">  key:</span> <span class="string">QVFCZUJlZFpPNE5kS1JBQXEvY1lNanA1SURiWklYcTIwS2tvSVE9PQ==</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">kubernetes.io/rbd</span></span><br></pre></td></tr></table></figure><p><code>data.key</code> is generated on one of the Ceph `MON` nodes using </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph auth get-key client.kube | base64</span><br></pre></td></tr></table></figure><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><h2 id="Deploy-rbd-provisioner"><a href="#Deploy-rbd-provisioner" class="headerlink" title="Deploy rbd provisioner"></a>Deploy rbd provisioner</h2><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">rbd-provisioner</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  strategy:</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">Recreate</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">rbd-provisioner</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">rbd-provisioner</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">"quay.io/external_storage/rbd-provisioner:latest"</span></span><br><span class="line"><span class="attr">        env:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">PROVISIONER_NAME</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">ceph.com/rbd</span></span><br><span class="line"><span class="attr">      serviceAccountName:</span> <span class="string">persistent-volume-binder</span></span><br></pre></td></tr></table></figure><p>Must specify<code>serviceAccountName</code> cause rbac was enabled</p><h2 id="Create-storageclass"><a href="#Create-storageclass" class="headerlink" title="Create storageclass"></a>Create storageclass</h2><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">rbd</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">storageclass.beta.kubernetes.io/is-default-class:</span> <span class="string">"true"</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">ceph.com/rbd</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line"><span class="attr">  monitors:</span> <span class="number">172.20</span><span class="number">.10</span><span class="number">.91</span><span class="string">:6789,172.20.10.96:6789,172.20.10.97:6789</span></span><br><span class="line"><span class="attr">  pool:</span> <span class="string">kube</span></span><br><span class="line"><span class="attr">  adminId:</span> <span class="string">admin</span></span><br><span class="line"><span class="attr">  adminSecretNamespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">  adminSecretName:</span> <span class="string">ceph-secret</span></span><br><span class="line"><span class="attr">  userId:</span> <span class="string">kube</span></span><br><span class="line"><span class="attr">  userSecretName:</span> <span class="string">ceph-user-secret</span></span><br><span class="line"><span class="attr">  imageFormat:</span> <span class="string">"2"</span></span><br><span class="line"><span class="attr">  imageFeatures:</span> <span class="string">layering</span></span><br></pre></td></tr></table></figure><h2 id="创建一个例子"><a href="#创建一个例子" class="headerlink" title="创建一个例子"></a>创建一个例子</h2><h3 id="pvc-yaml"><a href="#pvc-yaml" class="headerlink" title="pvc.yaml"></a>pvc.yaml</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">jenkins-home-claim</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="attr">    requests:</span></span><br><span class="line"><span class="attr">      storage:</span> <span class="number">10</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">  storageClassName:</span> <span class="string">rbd</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">jenkins-ref-claim</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="attr">    requests:</span></span><br><span class="line"><span class="attr">      storage:</span> <span class="number">10</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">  storageClassName:</span> <span class="string">rbd</span></span><br></pre></td></tr></table></figure><h3 id="deployment-svc-yaml"><a href="#deployment-svc-yaml" class="headerlink" title="deployment-svc.yaml"></a>deployment-svc.yaml</h3><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">jenkins:alpine-latest</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">          protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">        livenessProbe:</span></span><br><span class="line"><span class="attr">          httpGet:</span></span><br><span class="line"><span class="attr">            path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">            port:</span> <span class="number">9090</span></span><br><span class="line"><span class="attr">          initialDelaySeconds:</span> <span class="number">30</span></span><br><span class="line"><span class="attr">          timeoutSeconds:</span> <span class="number">30</span></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - mountPath:</span> <span class="string">/var/run/docker.sock</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">docker-sock</span></span><br><span class="line"><span class="attr">          readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">        - mountPath:</span> <span class="string">/var/jenkins_home</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">jenkins-home</span></span><br><span class="line"><span class="attr">        - mountPath:</span> <span class="string">/usr/share/jenkins/ref</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">jenkins-ref</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">docker-sock</span></span><br><span class="line"><span class="attr">        hostPath:</span></span><br><span class="line"><span class="attr">          path:</span> <span class="string">/var/run/docker.sock</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">jenkins-home</span></span><br><span class="line"><span class="attr">        persistentVolumeClaim:</span></span><br><span class="line"><span class="attr">          claimName:</span> <span class="string">jenkins-home-claim</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">jenkins-ref</span></span><br><span class="line"><span class="attr">        persistentVolumeClaim:</span></span><br><span class="line"><span class="attr">          claimName:</span> <span class="string">jenkins-ref-claim</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">NodePort</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">jenkins</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="number">8080</span></span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ceph </tag>
            
            <tag> rbd </tag>
            
            <tag> k8s </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>CephRBD集群安装</title>
      <link href="/2017/10/19/CephRBD%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/"/>
      <url>/2017/10/19/CephRBD%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/</url>
      <content type="html"><![CDATA[<h1 id="快速安装文档"><a href="#快速安装文档" class="headerlink" title="快速安装文档"></a><a href="http://docs.ceph.org.cn/start/quick-ceph-deploy/" target="_blank" rel="noopener">快速安装文档</a></h1><p>admin-node(ceph-deploy)</p><hr><h2 id="osd扩容"><a href="#osd扩容" class="headerlink" title="osd扩容"></a>osd扩容</h2><h3 id="用户创建等工作"><a href="#用户创建等工作" class="headerlink" title="用户创建等工作"></a>用户创建等工作</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">useradd ceph</span><br><span class="line">passwd ceph</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"ceph  ALL=(ALL) NOPASSWD: ALL"</span> | sudo tee /etc/sudoers.d/ceph</span><br></pre></td></tr></table></figure><h3 id="mon节点添加免密登录"><a href="#mon节点添加免密登录" class="headerlink" title="mon节点添加免密登录"></a>mon节点添加免密登录</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-copy-id node04</span><br><span class="line">ssh-copy-id node05</span><br></pre></td></tr></table></figure><h3 id="ceph-deploy"><a href="#ceph-deploy" class="headerlink" title="ceph-deploy"></a>ceph-deploy</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get --assume-yes -q --no-install-recommends install -o Dpkg::Options::=--force-confnew ceph ceph-osd ceph-mds ceph-mon radosgw</span><br></pre></td></tr></table></figure><p>之后</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph-deploy install node04 node05</span><br><span class="line">ceph-deploy osd prepare node04:/u01 node05:/u01</span><br><span class="line">ceph-deploy osd activate node04:/u01 node05:/u01</span><br><span class="line">ceph-deploy admin node04 node05</span><br></pre></td></tr></table></figure><p>都正常执行完的话<code>ceph -w</code>可以看到集群开始重新分配pg</p><h3 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h3><p><code>health HEALTH_WARN 1 requests are blocked &gt; 32 sec</code></p><p>有可能是在数据迁移过程中, 用户正在对该数据块进行访问, 但访问还没有完成, 数据就迁移到别的 OSD 中, 那么就会导致有请求被 block, 对用户也是有影响的，解决方法</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph@node01:~$ sudo ceph health detail</span><br><span class="line">HEALTH_WARN 1 requests are blocked &gt; 32 sec; 1 osds have slow requests</span><br><span class="line">1 ops are blocked &gt; 1048.58 sec on osd.1</span><br><span class="line">1 osds have slow requests</span><br><span class="line">ceph@node01:~$ sudo ceph osd tree</span><br><span class="line">ID WEIGHT  TYPE NAME       UP/DOWN REWEIGHT PRIMARY-AFFINITY </span><br><span class="line">-1 8.17778 root default                                      </span><br><span class="line">-2 0.90919     host node01                                   </span><br><span class="line"> 0 0.90919         osd.0        up  1.00000          1.00000 </span><br><span class="line">-3 0.90919     host node02                                   </span><br><span class="line"> 1 0.90919         osd.1        up  1.00000          1.00000 </span><br><span class="line">-4 0.90919     host node03                                   </span><br><span class="line"> 2 0.90919         osd.2        up  1.00000          1.00000 </span><br><span class="line">-5 2.72510     host node04                                   </span><br><span class="line"> 3 2.72510         osd.3        up  1.00000          1.00000 </span><br><span class="line">-6 2.72510     host node05                                   </span><br><span class="line"> 4 2.72510         osd.4        up  1.00000          1.00000</span><br></pre></td></tr></table></figure><p>可以看到是在osd.1上，在node02节点，在对应节点的重启osd服务</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo /etc/init.d/ceph restart osd.1</span><br></pre></td></tr></table></figure><p>在mon节点再执行<code>sudo ceph -w</code>可以看到集群已经恢复HEALTH_OK。</p><hr><h2 id="mon增加节点"><a href="#mon增加节点" class="headerlink" title="mon增加节点"></a><a href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/1.3/html/administration_guide/managing_cluster_size" target="_blank" rel="noopener">mon增加节点</a></h2><h3 id="ceph-admin节点修改ceph-conf文件"><a href="#ceph-admin节点修改ceph-conf文件" class="headerlink" title="ceph admin节点修改ceph.conf文件"></a>ceph admin节点修改ceph.conf文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[global]</span><br><span class="line">fsid = 53f0f4a4-8108-4373-a286-30866ea1d23c</span><br><span class="line"><span class="comment">#mon_initial_members = node01</span></span><br><span class="line"><span class="comment">#mon_host = 172.20.10.91</span></span><br><span class="line">mon_initial_members = node01, node06, node07</span><br><span class="line">mon_host = node01, node06, node07</span><br><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br><span class="line">osd pool default size = 2</span><br><span class="line"></span><br><span class="line">[mon.node01]</span><br><span class="line">public_addr = 172.20.10.91</span><br><span class="line"></span><br><span class="line">[mon.node06]</span><br><span class="line">public_addr = 172.20.10.97</span><br><span class="line"></span><br><span class="line">[mon.node07]</span><br><span class="line">public_addr = 172.20.10.98</span><br></pre></td></tr></table></figure><p>新增的mon节点为node06，node07</p><h3 id="更新集群中节点的ceph的配置"><a href="#更新集群中节点的ceph的配置" class="headerlink" title="更新集群中节点的ceph的配置"></a>更新集群中节点的ceph的配置</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#ceph-deploy --overwrite-conf config push &lt;ceph-node0 ceph-node1 ...&gt;</span></span><br><span class="line">ceph-deploy --overwrite-conf config push node01 node02 node03 node04 node05 node06 node07</span><br></pre></td></tr></table></figure><h3 id="添加monitor到cluster"><a href="#添加monitor到cluster" class="headerlink" title="添加monitor到cluster"></a>添加monitor到cluster</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#ceph-deploy mon add &lt;hostname&gt;</span></span><br><span class="line">ceph-deploy mon add node06</span><br><span class="line">ceph-deploy mon add node07</span><br></pre></td></tr></table></figure><p>并确保monitor已经加入</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ceph@node01:~$ sudo ceph quorum_status --format json-pretty</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"election_epoch"</span>: 14,</span><br><span class="line">    <span class="string">"quorum"</span>: [</span><br><span class="line">        0,</span><br><span class="line">        1,</span><br><span class="line">        2</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"quorum_names"</span>: [</span><br><span class="line">        <span class="string">"node01"</span>,</span><br><span class="line">        <span class="string">"node06"</span>,</span><br><span class="line">        <span class="string">"node07"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"quorum_leader_name"</span>: <span class="string">"node01"</span>,</span><br><span class="line">    <span class="string">"monmap"</span>: &#123;</span><br><span class="line">        <span class="string">"epoch"</span>: 3,</span><br><span class="line">        <span class="string">"fsid"</span>: <span class="string">"53f0f4a4-8108-4373-a286-30866ea1d23c"</span>,</span><br><span class="line">        <span class="string">"modified"</span>: <span class="string">"2017-12-28 12:08:17.138941"</span>,</span><br><span class="line">        <span class="string">"created"</span>: <span class="string">"2017-10-31 10:52:55.395319"</span>,</span><br><span class="line">        <span class="string">"mons"</span>: [</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"rank"</span>: 0,</span><br><span class="line">                <span class="string">"name"</span>: <span class="string">"node01"</span>,</span><br><span class="line">                <span class="string">"addr"</span>: <span class="string">"172.20.10.91:6789\/0"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"rank"</span>: 1,</span><br><span class="line">                <span class="string">"name"</span>: <span class="string">"node06"</span>,</span><br><span class="line">                <span class="string">"addr"</span>: <span class="string">"172.20.10.97:6789\/0"</span></span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"rank"</span>: 2,</span><br><span class="line">                <span class="string">"name"</span>: <span class="string">"node07"</span>,</span><br><span class="line">                <span class="string">"addr"</span>: <span class="string">"172.20.10.98:6789\/0"</span></span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h2 id="删除osd节点"><a href="#删除osd节点" class="headerlink" title="删除osd节点"></a><a href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/2/html/administration_guide/adding_and_removing_osd_nodes" target="_blank" rel="noopener">删除osd节点</a></h2><h3 id="检查集群容量"><a href="#检查集群容量" class="headerlink" title="检查集群容量"></a>检查集群容量</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo ceph df</span><br><span class="line"><span class="comment">#or</span></span><br><span class="line">sudo ceph osd df</span><br></pre></td></tr></table></figure><h3 id="临时disable-scrubbing"><a href="#临时disable-scrubbing" class="headerlink" title="临时disable scrubbing"></a>临时disable scrubbing</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo ceph osd <span class="built_in">set</span> noscrub</span><br><span class="line">sudo ceph osd <span class="built_in">set</span> nodeep-scrub</span><br></pre></td></tr></table></figure><h3 id="限制back-fill以及recovery"><a href="#限制back-fill以及recovery" class="headerlink" title="限制back-fill以及recovery"></a>限制back-fill以及recovery</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">osd_max_backfills = 1</span><br><span class="line">osd_recovery_max_active = 1</span><br><span class="line">osd_recovery_op_priority = 1</span><br></pre></td></tr></table></figure><p>配置在<code>/etc/ceph/ceph.conf</code></p><h3 id="移除ceph-osd"><a href="#移除ceph-osd" class="headerlink" title="移除ceph osd"></a>移除ceph osd</h3><h4 id="osd节点停止服务"><a href="#osd节点停止服务" class="headerlink" title="osd节点停止服务"></a>osd节点停止服务</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo systemctl <span class="built_in">disable</span> ceph-osd@&lt;osd_id&gt;</span><br><span class="line">sudo systemctl stop ceph-osd@&lt;osd_id&gt;</span><br></pre></td></tr></table></figure><p><code>&lt;osd_id&gt;</code>从ceph的管理节点<code>sudo ceph osd tree</code>可以查看到对应节点的ID</p><h4 id="在移除osd之前，需要先从存储集群中移出"><a href="#在移除osd之前，需要先从存储集群中移出" class="headerlink" title="在移除osd之前，需要先从存储集群中移出"></a>在移除osd之前，需要先从存储集群中移出</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo ceph osd out &lt;osd_id&gt;</span><br></pre></td></tr></table></figure><p>当执行完这个命令后，ceph会重新负载均衡集群并复制数据到其他的OSD节点</p><p>那么我们需要执行<code>sudo ceph -w</code>监视<code>rebalancing</code>的过程，等待集群重新恢复到<code>active+clean</code>的状态。</p><h4 id="从CRUSH-map中删除OSD，保证不再接受到数据"><a href="#从CRUSH-map中删除OSD，保证不再接受到数据" class="headerlink" title="从CRUSH map中删除OSD，保证不再接受到数据"></a>从CRUSH map中删除OSD，保证不再接受到数据</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo ceph osd crush remove osd.&lt;osd_id&gt;</span><br></pre></td></tr></table></figure><h4 id="删除OSD的认证key"><a href="#删除OSD的认证key" class="headerlink" title="删除OSD的认证key"></a>删除OSD的认证key</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo ceph auth del osd.&lt;osd_id&gt;</span><br></pre></td></tr></table></figure><p>上面的过程完成之后，最后移出osd</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo ceph osd rm &lt;osd_id&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> ceph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ceph </tag>
            
            <tag> rbd </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
